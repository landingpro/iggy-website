"use strict";(self.webpackChunkiggy_website=self.webpackChunkiggy_website||[]).push([[5754],{5091:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/06/06/connectors-runtime","metadata":{"permalink":"/blogs/2025/06/06/connectors-runtime","source":"@site/blog/2025-06-06-connectors-runtime.md","title":"Connectors runtime","description":"Extending Apache Iggy capabilities","date":"2025-06-06T00:00:00.000Z","tags":[],"readingTime":4.95,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Connectors runtime","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":[],"hide_table_of_contents":false,"date":"2025-06-06T00:00:00.000Z"},"unlisted":false,"nextItem":{"title":"Zero-copy (de)serialization","permalink":"/blogs/2025/05/08/zero-copy-deserialization"}},"content":"## Extending Apache Iggy capabilities\\n\\nIn the world of message streaming, connectors quite often play a crucial role in facilitating data exchange between different systems. While Apache Iggy remains the core messaging infrastructure, **focusing on extreme efficiency** (high throughput, low latency, and minimal resource consumption), it could also benefit from a more extensible architecture. This is where connectors come into play.\\n\\n\x3c!--truncate--\x3e\\n\\n## What are the connectors\\n\\nIf you\'ve ever used e.g. Apache Kafka or one of the Kafka-compatible solutions, such as Redpanda, you might\'ve already encountered the concept of connectors. For example, there\'s a dedicated [Connect API](https://kafka.apache.org/documentation/#connectapi) allowing you to create the custom connectors (plugins) for various data sources.\\n\\nTypically, connectors are designed to handle data ingestion and transformation tasks. They can be used to read data from external sources, transform it, and then write it to the message streaming system (**sources**), or the other way around (**sinks**) - fetch data from the message streaming system and write it to external services (e.g. databases, file systems, etc.).\\n\\nLet\'s say that we would like to get the real-time changes from a database (Postgres CDC for example) and send them to Apache Iggy, while performing some optional data transformation, such as filtering or enriching the data. Or, we might want to fetch data from Apache Iggy (e.g. produced by our custom services), transform it, and then push it further to the external indexer such as Elastic or Quickwit.\\n\\n**Instead of building all these pipelines from scratch as custom applications, we can leverage the power of connectors to simplify the process**. Simply download one of the existing plugins, configure it using the provided configuration files, and start using it - no need to write any code!\\n\\nHere\'s an example of a configuration file for the sink connector named `stdout`:\\n\\n```toml\\n# Required configuration for a sink connector\\n[sinks.stdout]\\nenabled = true\\nname = \\"Stdout sink\\"\\npath = \\"connectors/libiggy_connector_stdout_sink\\"\\n\\n# Collection of the streams from which messages are consumed\\n[[sinks.stdout.streams]]\\nstream = \\"example_stream\\"\\ntopics = [\\"example_topic\\"]\\nschema = \\"json\\"\\nbatch_size = 1000\\npoll_interval = \\"5ms\\"\\nconsumer_group = \\"stdout_sink_connector\\"\\n\\n# Custom configuration for the sink connector, deserialized to type T from `config` field\\n[sinks.stdout.config]\\nprint_payload = true\\n\\n# Optional data transformation(s) to be applied after consuming messages from the stream\\n[sinks.stdout.transforms.add_fields]\\nenabled = true\\n\\n# Collection of the fields transforms to be applied after consuming messages from the stream\\n[[sinks.stdout.transforms.add_fields.fields]]\\nkey = \\"message\\"\\nvalue.static = \\"hello\\"\\n```\\n\\n## Rust-based plugins\\n\\nSince Apache Iggy is implemented in Rust, it was an easy choice to implement the connectors in Rust as well. **This allows us to take advantage of the Rust\'s powerful type system and memory safety features**, ensuring that the connectors are reliable and efficient. Internally, we use **[dlopen2](https://github.com/OpenByteDev/dlopen2)** library to load the plugins during the runtime initialization - feel free to check how **[Arroyo](https://www.arroyo.dev/blog/rust-plugin-systems/)** uses it too.\\n\\nThanks to this approach, just like Iggy itself, the connector runtime (which is a separate process), is very lightweight and easy to deploy. **The runtime uses just a few MBs of memory on its own, while consuming minimal CPU resources.** Behind the scenes, we use the shared **[Tokio runtime](https://tokio.rs)** to manage the asynchronous tasks and events across all connectors, as well as the **[tracing](https://docs.rs/tracing/latest/tracing/)** crate for logging and tracing purposes.\\n\\nWhen running a simple benchmark based on the custom **[Quickwit sink](https://github.com/apache/iggy/tree/master/core/connectors/sinks/quickwit_sink)** connector, which pulls the data in real time from Iggy stream, does a basic data transformation (using the JSON payload format), and then pushes the data further to the Quickwit HTTP API, we observed that this plugin **can easily handle hundreds of thousands of messages per second, while using ~40MB of memory**. And keep in mind, that it\'s the very first (alpha) release of the connector runtime, so the performance will only improve over time.\\n\\nLast, but not least, It\'s quite easy to create your own connectors - simply implement either `Sink` or `Source` trait, compile the library and configure it in the runtime. Please check the **[repository](https://github.com/apache/iggy/tree/master/core/connectors/)** or the **[documentation](/docs/connectors/introduction)** for more details.\\n\\n```rust\\n#[async_trait]\\npub trait Sink: Send + Sync {\\n    /// Invoked when the sink is initialized, allowing it to perform any necessary setup.\\n    async fn open(&mut self) -> Result<(), Error>;\\n\\n    /// Invoked every time a batch of messages is received from the configured stream(s) and topic(s).\\n    async fn consume(\\n        &self,\\n        topic_metadata: &TopicMetadata,\\n        messages_metadata: MessagesMetadata,\\n        messages: Vec<ConsumedMessage>,\\n    ) -> Result<(), Error>;\\n\\n    /// Invoked when the sink is closed, allowing it to perform any necessary cleanup.\\n    async fn close(&mut self) -> Result<(), Error>;\\n}\\n```\\n\\n```rust\\n#[async_trait]\\npub trait Source: Send + Sync {\\n    /// Invoked when the source is initialized, allowing it to perform any necessary setup.\\n    async fn open(&mut self) -> Result<(), Error>;\\n\\n    /// Invoked every time a batch of messages is produced to the configured stream and topic.\\n    async fn poll(&self) -> Result<ProducedMessages, Error>;\\n\\n    /// Invoked when the source is closed, allowing it to perform any necessary cleanup.\\n    async fn close(&mut self) -> Result<(), Error>;\\n}\\n```\\n\\n## What\'s next?\\n\\nSince it\'s the very early release to showcase what\'s possible with the connector runtime, we plan to focus on improving the performance and stability of the runtime itself.\\n\\nMoreover, we plan to build more connectors, data transformations and schema encoders/decoders, to support the seamless transition between the different data formats and protocols.\\n\\nIt\'s also worth mentioning that the runtime uses one of the existing network protocols available via Rust SDK to connect to the Iggy server (so-called distributed mode). And while it might be the case for most of the deployments out there, we might also support e.g. UDS or some sort of IPC to allow the connectors deployment on the same machine, next to the streaming server.\\n\\nFinally, we would love to hear your feedback and suggestions on how to improve the runtime and connectors. Please feel free to open an **[issue](https://github.com/apache/iggy/issues)**, **[pull request](https://github.com/apache/iggy/pulls)** or start a **[discussion](https://github.com/apache/iggy/discussions)**.\\n\\nAs always, you are more than welcome to join our **[Discord community](https://discord.gg/C5Sux5NcRa)**!"},{"id":"/2025/05/08/zero-copy-deserialization","metadata":{"permalink":"/blogs/2025/05/08/zero-copy-deserialization","source":"@site/blog/2025-05-08-zero-copy-deserialization.md","title":"Zero-copy (de)serialization","description":"Introduction","date":"2025-05-08T00:00:00.000Z","tags":[],"readingTime":5.895,"hasTruncateMarker":true,"authors":[{"name":"Grzegorz Koszyk","title":"Apache Iggy PPMC Member","url":"https://github.com/numinnex","socials":{},"key":null,"page":null},{"name":"Hubert Gruszecki","title":"Apache Iggy PPMC Member","url":"https://github.com/hubcio","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Zero-copy (de)serialization","authors":[{"name":"Grzegorz Koszyk","title":"Apache Iggy PPMC Member","url":"https://github.com/numinnex"},{"name":"Hubert Gruszecki","title":"Apache Iggy PPMC Member","url":"https://github.com/hubcio"}],"tags":[],"hide_table_of_contents":false,"date":"2025-05-08T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Connectors runtime","permalink":"/blogs/2025/06/06/connectors-runtime"},"nextItem":{"title":"Transparent Benchmarking with Apache Iggy","permalink":"/blogs/2025/02/17/transparent-benchmarks"}},"content":"## Introduction\\n\\nApache Iggy considers **performance as one of its core principles**. We take pride in being blazingly fast, as proof of that, we have made **[benchmarking first-class citizen](https://iggy.apache.org/blogs/2025/02/17/transparent-benchmarks)**.\\n\\n**Zero-copy schema** was a natural next step in our high-performance journey, it was part of our roadmap for quite a while, until the day we have **[finally merged it](https://github.com/apache/iggy/pull/1679)**. In this blog post, we will share our path to implementing it.\\n\\n\x3c!--truncate--\x3e\\n\\n## Zero copy with rkyv\\n\\nOur initial approach to zero copy was leveraging the existing **[rkyv](https://github.com/rkyv/rkyv)** package which might or might not have been the best choice (more about it later). Using rkyv is pretty straightforward, as it revolves around three traits (`Archive`, `Serialize`, `Deserialize`), all three of which can be derived using derive macros, thus making our job pretty easy.\\n\\n```rust\\n#[derive(Archive, Deserialize, Serialize)]\\nstruct IggyBatch {\\n   pub start_offset: u64,\\n   pub start_timestamp: u64,\\n   pub end_offset: u64,\\n   pub end_timestamp: u64,\\n   pub batch_size: u64,\\n   pub attributes: u16,\\n   pub messages: Vec<IggyMessage>,\\n}\\n```\\n\\nSame traits are derived for `IggyMessage`\\n\\n```rust\\n#[derive(Archive, Deserialize, Serialize)]\\npub struct IggyMessage {\\n   pub offset: u64,\\n   pub timestamp: u64,\\n   pub value: Vec<u8>,\\n   pub headers: Vec<u8>,\\n}\\n```\\n\\nRkyv internally figures out the memory layout used for serialization, such that it can later on cast it back into its Archived form. (A couple bits of trivia: rkyv, when performing zero copy deserialization, turns the byte representation of your data into its Archived form, so instead of `IggyBatch` we receive `ArchivedIggyBatch`, rkyv has to reimplement certain complex types such as `Vec`, `HashMap` etc., and that\u2019s why it yields back your type in Archived form.)\\nYou can learn more about it by reading the **[rkyv book](https://rkyv.org)**, or from this **[presentation](https://www.youtube.com/watch?v=ON4z2LbTD-4)**.\\n\\nAfter a further evaluation of `IggyBatch`, we found that while it was good enough, there was still room for improvement.\\nImagine a scenario where the client sends a batch with 100 messages, the server receives it, turns it into Archived form, updates metadata fields, caches/persists it and sends an ack back to the client. The client sends a fetch request for 10 messages, the server receives the request, peeks into the cache, finds our batch of 100 messages and here is the tricky bit -\\nWe would like to send back **only a slice of the cached data** (10 messages instead of 100), but due to rkyv\'s memory layout, one cannot simply take a slice of those bytes and perform the transformation into its Archived form.\\n\\nSo rather than deriving rkyv\'s traits to `IggyBatch`, we decided to refine our approach by working at the individual message level.\\n\\n```rust\\nstruct IggyBatch {\\n   pub start_offset: u64,\\n   pub start_timestamp: u64,\\n   pub end_offset: u64,\\n   pub end_timestamp: u64,\\n   pub batch_size: u64,\\n   pub attributes: u16,\\n   pub messages: AlignedVec, // AlignedVec is custom type from rkyv, that represents a vector of bytes.\\n}\\n```\\n\\nWe redesigned the IggyBatch structure so that the `messages` field now stores a byte representation of `Vec<IggyMessage>`, with each message prefixed by its 4-byte length. This approach allows us to traverse through the data, cast individual messages to its archived form, and perform some work on it.\\n\\n![image](/zero-copy-deserialization/iggy_batch_schema_1.png)\\n\\nBy creating this frankenstein layout that combines length prefixes with rkyv\'s memory layout, we\'ve achieved the flexibility that we were aiming for, optimized for the most costly copies (messages), rather than the entire batch including its header.\\n\\nOne thing worth noting, rkyv requires buffer with your data to be aligned, going after rkyv book - \u201c16-aligned memory should be sufficient.\u201d, thus if you want to prefix your payload with any metadata, make sure that it doesn\u2019t misalign your buffer, or you can opt-in the `unaligned` feature.\\n\\n## Beyond rkyv\\n\\nRkyv is a pretty \u201cheavy\u201d crate that spreads through your entire application from network schema to disk schema, partially coupling your own versioning with its. **For many applications there is no way around this**, as it is the most complete zero-copy deserialization crate, but we decided to implement a more lightweight solution that gives us control over the memory layout.\\n\\nWe\u2019ve decided to still process messages individually, but have replaced length prefixes with a separate index vector.\\n\\n```rust\\npub struct IggyBatch {\\n   // Remaining fields...\\n   pub indexes: Vec<u8>,\\n   pub messages: Vec<u8>,\\n}\\n```\\n\\n![image](/zero-copy-deserialization/iggy_batch_schema_2.png)\\n\\nThis index vector consists of bytes that, when deserialized form `u32` values that point to specific positions within the message data. Separating messages from indexes serves purpose beyond the scope of this blog post, but shortly, this index structure closely resembles what we use on the server for message lookup in the log. This in turn, later on **allows us to efficiently reuse memory**.\\n\\nWhen iterating through the batch, **we yield views** instead of fully deserialized messages.\\n\\nIn case if we\u2019d need to have `IggyMessage` deserialized, we can do it just in time, rather than eagerly, for example when client processes a fetched batch from the server.\\n\\nThis approach allows us to combine the flexibility of solution with length prefixes using rkyv as well as having the freedom of not depending on a 3rd party crate, in fact this solution looks fairly similar to serde partial zero copy deserialization.\\n\\n## Benchmarks\\n\\nNow the part that all of you are probably most interested in - le benchmarks.\\n\\nThose results come from an **AWS i3en.3xlarge** instance\\n\\n**Before**\\n\\n![image](/zero-copy-deserialization/no_zero_copy_producer.png)\\n![image](/zero-copy-deserialization/no_zero_copy_consumer.png)\\n\\n| No zero-copy      |   tput   |   p95   |   p99   |   avg   |\\n| :---------------- | :------: | :-----: | :-----: | :-----: |\\n| Producer          |  1.7GB/s |  3.48ms | 4.23ms  | 2.39ms\\n| Consumer          |  2.1GB/s |  2.53ms | 2.93ms  | 1.90ms\\n\\n**After**\\n\\n![image](/zero-copy-deserialization/zero_copy_producer.png)\\n![image](/zero-copy-deserialization/zero_copy_consumer.png)\\n\\n| Zero-copy         |   tput   |   p95   |   p99   |   avg   |\\n| :---------------- | :------: | :-----: | :-----: | :-----: |\\n| Producer          |  2.4GB/s |  2.33ms | 2.59ms  | 1.63ms\\n| Consumer          |  4.0GB/s |  1.21ms | 1.46ms  | 0.98ms\\n\\n\\n**Almost 2 times higher throughput for reads (2,1GBps vs 4GBps), 40% higher throughput for writes (2,4GBps vs 1,7GBps), 2x better p99 latencies for reads (2.93ms vs 1,46 ms) and 63% better p99 latencies for writes (4.23ms vs 2.59ms).**\\n\\nYou can find more detailed comparisons on our **[benchmarking website](https://benchmarks.iggy.rs)**.\\n\\n## Concluding\\n\\n**It was a lot of fun exploring the entire design space for our zero copy schema**. Depending on the constraints one might end up with a completely different solution, in fact zero copy isn\'t always the optimal approach, for example, if you\'d like to read more about challenges that folks in embedded development have to overcome, check out **[this Bluesky post from James Munns](https://bsky.app/profile/jamesmunns.com/post/3lnqo5ykawc2r)**, or more broadly his project **[postcard-rpc](https://github.com/jamesmunns/postcard-rpc)**.\\n\\n## What\u2019s next for Iggy\\n\\nImplementing zero-copy was a major milestone for us, but the train doesn\u2019t slow down. Next in line is **io_uring** and **thread per core shared nothing** architecture. **[We\u2019ve already promised](https://www.reddit.com/r/rust/comments/1d35fsb/comment/l65wgnt/)** that we will publish more details about it once we have the results from our benchmark tests, so expect a highly technical blog post in near future.\\n\\nFor now **[we did a proof of concept long time ago](https://github.com/apache/iggy/tree/io_uring_monoio_runtime)**, but realised a few things along the way that we would like to do differently. So we will. Iggy moves onward and upward! \ud83d\ude80"},{"id":"/2025/02/17/transparent-benchmarks","metadata":{"permalink":"/blogs/2025/02/17/transparent-benchmarks","source":"@site/blog/2025-02-17-transparent-benchmarks.md","title":"Transparent Benchmarking with Apache Iggy","description":"Benchmarks should be the first-class citizen","date":"2025-02-17T00:00:00.000Z","tags":[],"readingTime":3.955,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Transparent Benchmarking with Apache Iggy","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":[],"hide_table_of_contents":false,"date":"2025-02-17T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Zero-copy (de)serialization","permalink":"/blogs/2025/05/08/zero-copy-deserialization"},"nextItem":{"title":"Iggy joins the Apache Incubator","permalink":"/blogs/2025/02/10/apache-incubator"}},"content":"## Benchmarks should be the first-class citizen\\n\\nIn the world of software development, **benchmarks are often treated as a second-class citizen**. They\'re more of an addition to the codebase, rather than a crucial part of it, which should be the other way around, especially when it comes to the performance-critical systems or infrastructure tools.\\n\\n\x3c!--truncate--\x3e\\n\\nSometimes, the benchmarking results are nothing more than just a **cherry-picking of the best-case scenarios**, which are not representative of real-world usage. In such a case, they simply serve a sole purpose of either making the project look better than it is or how well it does outperform the competition, under the extremely optimized conditions when comparing with its counterparts.\\n\\n**Trying to reproduce the benchmarks is often a nightmare**, as the environment setup is not documented, the code is unavailable, or the instructions are not clear enough. This makes it close to impossible to verify the results, which are then taken for granted.\\n\\nOr even worse, the **benchmarking tool might be so complex, that it\'s hard to understand how it works**, and what are the assumptions behind it. ALl of these, does result in hard to extend or modify the existing benchmarks, which are not covering the particular use case you\'re interested in. It\'s just here to tell everyone that we do have benchmarks, but how we do it, and what they measure, is a mystery.\\n\\n**Which is why at [Iggy](https://github.com/apache/iggy/), we\'ve decided to make the benchmarks a first-class citizen**.\\n\\nOur `iggy-bench` tool, which is used to run the benchmarks and is part of the core open source repository (can be found under the `bench` directory), has come a long way and has been serving us well.\\n\\n![image](/transparent-benchmarks/iggy_bench_cli.png)\\n\\nWe use it to do quick performance checks, regression testing, and to see how the changes we introduce affect the performance. **We run it on our localhost, as well as on the Virtual Machines in the cloud, to see how it behaves under a variety of environments.**\\n\\n## Iggy benchmarking dashboard\\n\\n**And today, we\'re proud to present [benchmarks.iggy.rs](https://benchmarks.iggy.rs/)** - a benchmarking dashboard, which is available to everyone. It\'s a website where you can see how Iggy performs under the different conditions, and how it scales with the number of clients, messages, and topics.\\n\\nThis is our community-driven effort, where everyone can contribute, and add their own benchmarks. For all the information on how to run the benchmarks, render them on the dashboard, upload your results or contribute to the project, please check the [iggy-bench-dashboard](https://github.com/iggy-rs/iggy-bench-dashboard) repository. In general, it\'s as simple as:\\n\\n- Building the [Iggy](https://github.com/apache/iggy) in the release mode with `cargo build --release`\\n- Starting your Iggy server with `cargo r --bin iggy-server -r` (feel free to adjust the configuration in `server.toml` or via environment variables)\\n- Running the `iggy-bench` tool with the desired parameters, e.g. `cargo r --bin iggy-bench -r pinned-producer tcp`\\n- Extending your benchmark with the output (HTML charts, JSON sampling etc.) `cargo r --bin iggy-bench -r pinned-producer tcp output -o performance_results --identifier spetz`\\n- Navigating to the specific benchmark directory to browse the charts and/or uploading them to the dashboard.\\n- And there\'s always `help` command e.g. `pinned-producer --help` to make your life easier :)\\n\\n![image](/transparent-benchmarks/iggy_bench_dashboard.png)\\n\\n**And this is just the beginning**, as we plan to extend the dashboard, and add more benchmarks, which are covering the different use cases.\\n\\nOur main goal is to make the benchmarking process (and its results) **transparent, reproducible, and easy to understand**. We want to make them a first-class citizen, and a crucial part of the Iggy project. We want to make them a tool, which will help us to improve the performance, and to make Iggy the best streaming server out there. We\'re looking forward to your feedback, and we hope you\'ll enjoy the benchmarks.\\n\\n## Towards the microsecond latency\\n\\n**And as a cherry on top, we\'ve recently managed to achieve the sub-millisecond write latency**. This is a huge milestone for us, as it\'s a proof that Iggy can be used in low-latency applications, where speed is crucial. Lately, we\'ve been experimenting a lot with [rkyv](https://github.com/rkyv/rkyv) - zero-copy deserialization framework, which has yielded some great results. Keep in mind that streaming the data within the range of microseconds latency depends on the several factors, such as message size, network conditions, or the hardware you\'re running on.\\n\\nAnd the best part is that we\'re just getting started. We\'re looking forward to pushing the limits even further, and to see how far we can go. There\'s still tons of optimizations coming, including switching the runtime to the [monoio](https://github.com/bytedance/monoio) which does support **io_uring**, and we\'ve experienced superb results with this one on our experimental branch. Then, there\'s the whole concept of shared-nothing & thread-per-core design, and many more. Stay tuned!"},{"id":"/2025/02/10/apache-incubator","metadata":{"permalink":"/blogs/2025/02/10/apache-incubator","source":"@site/blog/2025-02-10-apache-incubator.md","title":"Iggy joins the Apache Incubator","description":"We are thrilled to announce that Iggy has officially joined the Apache Incubator! This marks a major milestone in our journey to redefine message streaming \u2014 one that is blazingly fast, hyper-efficient, and built for the future. Since the very first day, Iggy was always meant to be a truly FOSS project \u2014 not just open-source in name, but deeply rooted in the values of transparency, collaboration, and community-driven innovation.","date":"2025-02-10T00:00:00.000Z","tags":[],"readingTime":3.46,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Iggy joins the Apache Incubator","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":[],"hide_table_of_contents":false,"date":"2025-02-10T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Transparent Benchmarking with Apache Iggy","permalink":"/blogs/2025/02/17/transparent-benchmarks"},"nextItem":{"title":"Iggy.rs - Technology Radar & current goals","permalink":"/blogs/2024/10/28/technology-radar-and-currrent-goals"}},"content":"We are thrilled to announce that **Iggy** has officially joined the **[Apache Incubator](https://incubator.apache.org/)**! This marks a major milestone in our journey to redefine message streaming \u2014 one that is **blazingly fast, hyper-efficient, and built for the future**. Since the very first day, **Iggy was always meant to be a truly FOSS project** \u2014 not just open-source in name, but deeply rooted in the values of transparency, collaboration, and community-driven innovation.\\n\\n\x3c!--truncate--\x3e\\n\\n## \ud83d\ude80 A Journey Two Years in the Making\\nIt\u2019s been almost two years since the very first line of code was written, and we\u2019ve come a long way since the initial release. What started as a bold idea has grown into a high-performance, cost-efficient message streaming solution with a thriving community of contributors, users, and advocates. We\u2019ve received invaluable feedback, contributions, and support, and we\u2019re more excited than ever to take Iggy to the next level.\\n\\n\\n## \ud83d\udd25 Beyond Just Iggy \u2013 A Growing Ecosystem\\nIggy is no longer just about the core repository. Over time, we\u2019ve built an entire ecosystem around it, including:\\n\\n- \u2705 SDKs for seamless developer integrations\\n- \u2705 CLIs for powerful command-line control\\n- \u2705 Web UIs for intuitive management\\n- \u2705 And many more\u2026\\n\\nWe\u2019re already approaching 20 repositories under the Iggy organization ([see them here](https://github.com/iggy-rs/)), and as we enter the Apache Incubator, we are committed to ensuring that all of them follow ASF guidelines while maintaining our core values of speed, efficiency, and simplicity.\\n\\n## \ud83d\udd17 Why Apache?\\n\\nThe Apache Software Foundation has nurtured some of the **most transformative open-source projects in history** \u2014 Hadoop, Spark, Lucene, Solr, Cassandra, Kafka, Flink, and Airflow \u2014 technologies that have **shaped big data, search, and real-time processing**. By joining Apache Incubator, we align with this legacy and **gain access to a larger community, better governance, and long-term sustainability**.\\n\\n\ud83d\udd17 Dive into the Apache Incubator Details\\n\\n- [Official Proposal](https://cwiki.apache.org/confluence/display/INCUBATOR/Iggy+Proposal)\\n- [Discussion Thread](https://lists.apache.org/thread/q9whr3q9qd6wqm89f0vc1f6vkkvzc8xf)\\n- [Voting Results](https://lists.apache.org/thread/6zfgdjwrzs92h4z4x6b25v1r23g3f5yg)\\n\\n\\n## \ud83d\udcc8 What\u2019s Next?\\n**Our vision is clear: Iggy as a future Apache Top-Level Project (TLP)**. We already have ambitious ideas on how to improve both the project and the community around it:\\n\\n- **Scaling performance benchmarks** to push the limits of ultra-low-latency streaming\\n- **Expanding integrations** with modern data infrastructure\\n- **Building a vibrant developer ecosystem** that makes message streaming frictionless\\n\\n## \ud83e\udd80 Codebase transition\\n\\nAfter the recent [discussion](https://lists.apache.org/thread/zrn96nlg23r9353lr5tp2by2ggx4zxqc), we plan to stick to the **monorepo approach**, under which, we will have a single repository for all the Iggy-related projects. This will make it easier for the contributors to navigate through the codebase, and to see how the changes in one project affect the others. This should also help us to keep the consistency across the projects, especially, once we release the Rust bindings to be used within the other languages SDKs.\\n\\nAll the repositories under [iggy-rs](https://github.com/iggy-rs) will be eventually moved to the [apache/iggy](https://github.com/apache/iggy) repository, and we will make sure that all the existing links and references are updated accordingly.\\n\\nWe\'ll also host our website under the [iggy.apache.org](https://iggy.apache.org) domain, including the documentation, blog, and the upcoming benchmarks. We\'ve also updated our social media handles to contain the Apache prefix.\\n\\nThere will be most likely some changes related to hosting the Docker images, as well as the other tools we\'re using, but we\'ll make sure to keep you updated on that.\\n\\n## \ud83d\udca1 Join the Movement\\nIf you believe in a future where message streaming is lightning-fast, hyper-efficient, and accessible to all, we invite you to be part of this journey. Whether you\'re a developer, architect, or enterprise innovator, your contributions, feedback, and ideas will shape what comes next.\\n\\nAnd this is just the beginning. **Welcome to the era of Hyper-Efficient Message Streaming at Laser Speed.**\\n\\n**Last but not least, we\'ve got a new logo**! Our lovely Italian Greyhound is now a part of the Apache family, and we\'re proud to have it as our mascot. As you can see, it\'s so fast, that even the light travelling through the optical fiber can\'t keep up with it :)\\n\\n![image](/apache-incubator/iggy_logo.png)\\n\\n\ud83d\udc49 Follow us, contribute, and help to build the future of the message streaming!\\n\\n- [Discord](https://discord.gg/C5Sux5NcRa)\\n- [X (Twitter)](https://x.com/ApacheIggy)\\n- [Bluesky](https://bsky.app/profile/iggy.rs)\\n- [LinkedIn](https://www.linkedin.com/company/apache-iggy/)"},{"id":"/2024/10/28/technology-radar-and-currrent-goals","metadata":{"permalink":"/blogs/2024/10/28/technology-radar-and-currrent-goals","source":"@site/blog/2024-10-28-technology-radar-and-currrent-goals.md","title":"Iggy.rs - Technology Radar & current goals","description":"Technology Radar","date":"2024-10-28T00:00:00.000Z","tags":[],"readingTime":6.58,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Iggy.rs - Technology Radar & current goals","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":[],"hide_table_of_contents":false,"date":"2024-10-28T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Iggy joins the Apache Incubator","permalink":"/blogs/2025/02/10/apache-incubator"},"nextItem":{"title":"Iggy.rs \u2014 one year of building the message streaming","permalink":"/blogs/2024/05/29/one-year-of-building-the-message-streaming"}},"content":"## Technology Radar\\n\\nQuite recently (a few days ago), [Iggy](https://github.com/apache/iggy) has been listed on [Technology Radar](https://www.thoughtworks.com/radar/platforms/summary/iggy) by [Thoughtworks](https://www.thoughtworks.com/) - a well-known technology consulting company.\\n\\nIf you\'re not familiar with the Technology Radar, it\'s essentially an opinionated set (updated twice a year and subscribed by the thousands of developers worldwide) of the tools, platforms, frameworks, techniques etc. which you may want to try out & explore in your IT projects. Everything is split into the different categories, depending on the maturity or popularity of the particular tool.\\n\\n\x3c!--truncate--\x3e\\n\\n![image](/technology-radar-and-currrent-goals/iggy_technology_radar.png)\\n\\nAs you can see, we were put right into the **assess** bucket (next to such renowned solutions such as e.g. [FoundationDB](https://www.foundationdb.org/)) - being the projects which are worth exploring & understanding how they might affect your enterprise. Frankly speaking, we weren\'t expecting this at all, and **from our perspective, it\'s quite of an accomplishment**.\\n\\nBesides gaining an additional amount of trust & recognition, it has led us to another conclusion - someone out there we don\'t know yet about (maybe even one of their customers) is using/experimenting with Iggy :)\\n\\nAnd if you are (or will be) one of such persons, please hop onto our [Discord](https://discord.gg/C5Sux5NcRa) and share your invaluable feedback with us!\\n\\nNow, given the recent publication and increased activity within our [OSS community](https://github.com/iggy-rs/) building the core streaming server & SDKs in multiple programming languages, it\'s worth mentioning what are the current goals for Iggy.\\n\\n## Current goals\\n\\n### Replication\\n\\nWithout a doubt, being able to run your infrastructure (which processes & stores the data) as a cluster, gives much more confidence and greatly impacts the overall reliability.\\n\\nWe\'ve started [experimenting](https://github.com/iggy-rs/iggy-cluster-sandbox) with the replication over half a year ago already by implementing the basic, Raft based consensus algorithm for the simple message streaming server.\\n\\nAt the same time, we were researching the other possible solutions, after we\'ve finally decided to move on with **Viewstamped Replication** (in its [revisited form](https://pmg.csail.mit.edu/papers/vr-revisited.pdf)), which was successfully used by e.g. [TigerBeetle](https://tigerbeetle.com/).\\n\\nLong story short - the **deterministic leader election**, allows us to go for ring topology and chain replication of our data - it\'s excellent for high throughput, which is very important for us.\\n\\nMoreover, **VSR can be run completely in memory**, providing us an opportunity to work independently both on the consensus and the storage and how to link these two together, to form a bulletproof storage fault model.\\n\\nBelow is our very first draft for the initial implementation of VSR.\\n\\n![image](/technology-radar-and-currrent-goals/iggy_vsr.png)\\n\\n### S3 storage\\n\\nA few months ago, we did [implement](https://github.com/apache/iggy/pull/1053) an optional archiver for the server state log & streaming data (messages etc.) which supports any S3 compatible storage (just pick up your favorite cloud provider). The configuration is as simple as this example:\\n\\n```toml\\n[data_maintenance.archiver]\\n# Enables or disables the archiver process.\\nenabled = true\\n\\n# Kind of archiver to use. Available options: \\"disk\\", \\"s3\\".\\nkind = \\"s3\\"\\n\\n[data_maintenance.archiver.disk]\\n# Path for storing the archived data on disk.\\npath = \\"local_data/archive\\"\\n\\n[data_maintenance.archiver.s3]\\n# Access key ID for the S3 bucket.\\nkey_id = \\"123\\"\\n\\n# Secret access key for the S3 bucket\\nkey_secret = \\"secret\\"\\n\\n# Name of the S3 bucket.\\nbucket = \\"iggy\\"\\n\\n# Endpoint of the S3 region.\\nendpoint = \\"http://localhost:9000\\"\\n\\n# Region of the S3 bucket.\\nregion = \\"eu-west-1\\"\\n\\n# Temporary directory for storing the data before uploading to S3.\\ntmp_upload_dir = \\"local_data/s3_tmp\\"\\n```\\n\\n**By making use of S3, you could almost infinitely (and very cheaply) store your data** - for the need of additional backups, being compliant with law regulations etc. However, there\'s one catch - in order to read the data stored with S3, you\'d need to download it from the cloud and restart your server. And this is where things will change in the future - we\'re planning to implement a dedicated S3 storage, for both, writing and reading the data in real-time if needed. You could think of the following analogy to the different kinds of cache storages in your PC.\\n\\n- **L1** - data available directly from the server RAM (super fast writes/reads)\\n- **L2** - data stored on your servers disks (still very, very fast with NVME SSD gen4 or 5)\\n- **L3** - S3 storage, still fast for the typical use-cases which do not require a very stable, microsecond level latencies\\n\\n**Each of these storage layers could be optionally enabled or disabled**. You can already decide if and how much memory to use for caching the messages. With S3 tiered storage in place, you could e.g. treat your server\'s SSD as a sort of ring buffer for keeping the most recent data (easily millions or billions of messages, depending on their size) and only fetch the ones from S3, when you need something very old.\\n\\nOr, you could just ignore your server\'s RAM & SSD, and do all the writes and reads directly on S3, and still remain blazingly fast (just like [Quickwit](https://quickwit.io)).\\n\\n### OpenTelemetry\\n\\nSpeaking of the Quickwit, we\'ve also [implemented](https://github.com/apache/iggy/pull/1294) a support for [OpenTelemetry](https://opentelemetry.io/) logs & traces for the server. Since our SDK already uses the logging & tracing libraries, we thought that adding such a feature on the server, could help you gain even better, real-time observability into what\'s happening under the hood.\\n\\n```toml\\n# OpenTelemetry configuration\\n[telemetry]\\n# Enables or disables telemetry.\\nenabled = false\\n# Service name for telemetry.\\nservice_name = \\"iggy\\"\\n\\n# OpenTelemetry logs configuration\\n[telemetry.logs]\\n# Transport for sending logs. Options: \\"grpc\\", \\"http\\".\\ntransport = \\"grpc\\"\\n# Endpoint for sending logs.\\nendpoint = \\"http://localhost:7281/v1/logs\\"\\n\\n# OpenTelemetry traces configuration\\n[telemetry.traces]\\n# Transport for sending traces. Options: \\"grpc\\", \\"http\\".\\ntransport = \\"grpc\\"\\n# Endpoint for sending traces.\\nendpoint = \\"http://localhost:7281/v1/traces\\"\\n```\\n\\n**And just like with S3 storage, it\'s merely a beginning** - one of the members on our [Discord](https://discord.gg/C5Sux5NcRa) had already thought of extending this implementation by **propagating the trace context** (via existing message headers metadata) between the clients & server in order to get full understanding of the distributed systems and its dependencies, which could be further visualized by tools like [Zipkin](https://zipkin.io/) or [Jaeger](https://www.jaegertracing.io/).\\n\\n### Optimizations\\n\\nImproved messages batching, keeping the indexes & time indexes in a single file, making use of **mmap** or **directIO** for the data storage processing, [rkyv](https://github.com/rkyv/rkyv) for **zero-copy (de)serialization**, keeping open the file descriptors and lots of other minor improvements - all these low hanging fruits (or at least some of them), will hopefully build up to making Iggy even more performant & resource effective than it already is.\\n\\n**To start the Iggy server, you just need to wait for a few milliseconds, and the RAM consumption is within a range ~20 MB, which is already over an order of magnitude lower than when compared to Kafka.**\\n\\n![image](/technology-radar-and-currrent-goals/iggy_docker.png)\\n\\n### io_uring\\n\\nThis will certainly require to have its own blog post, as there\'s so much to talk about. We did experiment with [Monoio](https://github.com/bytedance/monoio) (which, in its basic form without additonal enhancements allowed us to reach **over 15 GB/s reads** when compared to 10-12 GB/s for Tokio that we currently use), we also might experiment with [Glommio](https://github.com/DataDog/glommio), yet, most likely, we might **build our own io_uring backend** to fully utilize all its features.\\n\\nYes, at this point you might call us crazy (**io_uring** won\'t happen before we release the first version of the VSR clustering anyway), but if you want to tick all the possible boxes, it\'s hard to find a generic framework that will meet your demands, especially when mixed altogether with VSR clustering, thread-per-core & shared-nothing design (if will turn out to be suitable), zero-copy deserialization libraries and other things we might even not be aware of yet.\\n\\n**To innovate, one must experiment**, and although we do all these things in our spare time, it\'s been an exciting journey so far (and lots of experience gained in the meantime) for all of our team members building something fresh, from the very ground up, and regardless of the final outcome, we already know it was all worth it :)"},{"id":"/2024/05/29/one-year-of-building-the-message-streaming","metadata":{"permalink":"/blogs/2024/05/29/one-year-of-building-the-message-streaming","source":"@site/blog/2024-05-29-one-year-of-building-the-message-streaming.md","title":"Iggy.rs \u2014 one year of building the message streaming","description":"Throwback","date":"2024-05-29T00:00:00.000Z","tags":[],"readingTime":10.23,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Iggy.rs \u2014 one year of building the message streaming","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":[],"hide_table_of_contents":false,"date":"2024-05-29T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Iggy.rs - Technology Radar & current goals","permalink":"/blogs/2024/10/28/technology-radar-and-currrent-goals"},"nextItem":{"title":"Iggy.rs - building message streaming in Rust","permalink":"/blogs/2023/12/29/building-message-streaming-in-rust"}},"content":"## Throwback\\n\\n**It\'s been a little over a year**, since the [Iggy.rs](https://iggy.rs) was born. The initial idea of building a side project (as a way of studying Rust) \u2014 an infrastructure for the message streaming (think of Kafka, RedPanda, Aeron etc.) \u2014 eventually turned out to be something much bigger that I could\'ve ever imagined. In the [previous post](https://blog.iggy.rs/posts/building-message-streaming-in-rust/) (from almost half a year ago), I did describe what\'s [Iggy.rs](https://github.com/apache/iggy) all about, how it started, what\'s the ecosystem around it, what our goals are etc.\\n\\n\x3c!--truncate--\x3e\\n\\nThis **particular article** turned out to be a sort of **catalyst**, as it received a really nice traction on [Reddit](https://www.reddit.com/r/rust/comments/18tgubd/iggyrs_building_message_streaming_in_rust/), and was also mentioned on the main site of [Hacker News](https://news.ycombinator.com/item?id=38868115), which I do believe were the two main reasons for the growing interest & community since then. At this point, I\'d like to **thank you all very much** for such a kind feedback \u2014 honestly, during the very first weeks of 2024, there were so many things happening on our [Discord](https://discord.gg/C5Sux5NcRa), that with the rest of the team, we sometimes had a feeling as if we were providing enterprise premium support \u2014 really cool stuff!\\n\\nAnd although it may seem as if the project development has recently slowed down a bit, I\'d say it\'s quite the opposite \u2014 let me quickly summary, what we\'ve achieved so far during the last few months and what we\'re focusing on now, as the future looks bright :)\\n\\n![image](/one-year-of-building-the-message-streaming/iggy_stars.png)\\n\\n## Community\\n\\nFirst and foremost, if it weren\'t for the community, we wouldn\'t have seen such an enormous growth of the Iggy\'s ecosystem \u2014 we\'ve received the dozens of pull requests and there\'s ~250 members on our [Discord](https://discord.gg/C5Sux5NcRa). Whether we talk about bug fixes, improvements, new features, or just sharing the experiences and discussing potential ideas \u2014 it\'s all equally important.\\n\\nAnd it\'s been even more than that \u2014 we\'ve seen our community members take on **building the new SDKs** in their favorite programming languages, fully on their own. Today, you can find the following list of supported SDKs for Iggy.rs \u2014 some of them could be lagging behind, but it\'s expected, as the project is still evolving, and it\'s not an easy task, to come up with a great development experience from the very beginning.\\n\\n- [Rust](https://github.com/apache/iggy)\\n- [C#](https://github.com/iggy-rs/iggy-dotnet-client)\\n- [C++](https://github.com/iggy-rs/iggy-cpp-client)\\n- [Node (TS)](https://github.com/iggy-rs/iggy-node-client)\\n- [Go](https://github.com/iggy-rs/iggy-go-client)\\n- [Python](https://github.com/iggy-rs/iggy-python-client)\\n- [Java](https://github.com/iggy-rs/iggy-java-client)\\n- [Elixir](https://github.com/iggy-rs/iggy-elixir-client)\\n\\n## Changelog\\n\\nAdding the brand new SDKs wasn\'t the only great thing that has happened during last few months. We\'ve also made quite a lot of improvements for the streaming server itself:\\n\\n- Increased the streaming server throughput by over 30% for both writes and reads\\n- Added messages compression for client-side & server-side supporting the different algorithms\\n- Implemented a new way of message batching with an additional tooling for data migration\\n- Fixed the possible server-side deadlock that could happen for a specific configuration\\n- Fixed issues with possible memory leaks when storing too many indices in memory\\n- Rebuilt our custom benchmarking tool\\n- Improved TCP connection handling\\n- Constantly upgrading our CI/CD with lots of testing, different runtimes, artifacts, crates and Docker images releases\\n- Refactored the existing Rust client SDK to follow the new conventions (without the breaking changes to the previous one)\\n\\nAnd at the same time, we\'ve been experimenting a lot with some fancy stuff, which you can read about in the last paragraphs :)\\n\\n![image](/one-year-of-building-the-message-streaming/iggy_perf.png)\\n\\n## Tooling\\n\\nThe core message streaming server and multiple SDKs might sound as the most important parts of the whole ecosystem, but let\'s not forget about the **management tools**. How to quickly connect to the server, create new topics, validate if the messages are being sent correctly, change the user permissions or check the node statistics?\\n\\nThis is where our **[CLI](https://github.com/apache/iggy/tree/master/core/cli) and [Web UI](https://github.com/iggy-rs/iggy-web-ui) come in handy**. If you\'re a fan of working with the terminal and used to the great developer experience, you\'ll find our CLI a joy to work with.\\n\\n![image](/one-year-of-building-the-message-streaming/iggy_cli.png)\\n\\nOn the other hand, if you prefer a graphical interface accessible via your browser, Web UI has got you covered. What\'s even more impressive, is that both of these tools have been developed by the single developers.\\n\\n![image](/one-year-of-building-the-message-streaming/iggy_web.png)\\n\\nLast but not least, in order to run the benchmarks, we have our own [bench](https://github.com/apache/iggy/tree/master/core/bench) available as a part of the core repository \u2014 you can easily configure the number of producers, consumers, streams, etc. and get an overview of the possible streaming performance on your machine.\\n\\n![image](/one-year-of-building-the-message-streaming/iggy_bench.png)\\n\\n## Early adopters\\n\\nOverall, coding and implementing new features is one side of the story, the other is making an actual use of it. You might have the most sophisticated/performant/reliable (you name it) tooling out there, however, if no one is using it or at least experimenting with it, how could you possibly know whether it\'s even worth an effort to continue with the further development of the project? Well, I truly wish I had an easy answer how to find users willing to play with your new shiny toy.\\n\\nIn our case, I do believe, that it was a mix of two things \u2014 a limited amount of such tooling in the Rust ecosystem (so that the language enthusiasts could to try out something fresh), as well as a much more lightweight and (hopefully) performant message streaming infrastructure than some of the well-established solutions.\\n\\nI\'m fully aware that it\'s a bold claim, and running the synthetic benchmarks is not a viable proof (e.g. on my 7950X, I was able to hit **3GB/s writes and up to 10 GB/s reads** with some additional caching enabled), yet, most of our early adopters were very happy with their results e.g. outnumbering Kafka while utilizing much less memory. For example, [Marvin](https://github.com/apache/iggy/issues/606) wrote:\\n\\n> 20 million msg/sec via tcp is pretty nuts and already blows several commercial systems out of the water.\\n\\nAnd he\'s not the only one who found Iggy.rs to be the right tool for his needs. For example, a few days ago, one of the users on our Discord said that thanks to Iggy he was able to **achieve 2ms latency when compared to 300ms** with Kafka. Again, just to make it clear, I\'m not saying that we\'re better than X or Y \u2014 I\'m simply stating that for some specific usages, we might be a better choice than X or Y.\\n\\n## Clustering & replication\\n\\nOne of the most frequent questions we receive is whether we plan to incorporate some sort of data replication feature. Without any doubts, especially when considering the general system resiliency and reliability, being able to spin up a cluster of the particular piece of infrastructure (database/messaging/streaming/logging etc.) is quite often a critical feature.\\n\\nAnd yes, we will certainly implement clustering in Iggy \u2014 as a matter of fact, we\'ve already built its basic version in the [sandbox repository](https://github.com/iggy-rs/iggy-cluster-sandbox). In order to achieve that, we\'ve decided to (at least for now) stick to the [Raft](https://raft.github.io) consensus algorithm. However, adding the data replication feature to the core Iggy project will require a new way of storing the server metadata (most likely in a way of event-sourced messages, to play nicely with the replication between the nodes) and one more \\"tiny\\" thing regarding the overall I/O.\\n\\n![image](/one-year-of-building-the-message-streaming/iggy_cluster.png)\\n\\n## io_uring & thread-per-core + share-nothing\\n\\nWhat is a message streaming server, besides some more-or-less complicated logic regarding topics, partitioning, message ordering, consumer groups and a few more features? In its very core, **it\'s mostly I/O (disk + networking)** \u2014 the more efficient you can make it, the greater it will be. Of course, there\'s a major difference between throughput and latency, especially when talking about the so-called tail latency (p99 and more). Wouldn\'t it be great if we could have a very high throughput and very stable/predictable (and low) latency at the same time? This is exactly what we\'re currently trying to achieve. Originally, we\'ve started with the most popular Tokio runtime, which uses the work-stealing approach, and it\'s actually quite impressive (based on the benchmarks and the experiences shared by our early adopters).\\n\\nHowever, due to the nature of tasks being shared across the different threads, **you can\'t simply avoid lots of context switches and data being shared & synchronized across these threads** (therefore even more context switches will occur). While it\'s probably not an issue for like 90% or maybe even 99% use-cases, there might be some (financial systems and similar), where an unpredictable tail latency is a total no-go.\\n\\nCertainly, there are already existing solutions dealing with such challenges, such as [Aeron](https://aeron.io), but **we do believe, that we can make Iggy something much easier to use** \u2014 one that can handle the typical workloads, as well as the very demanding ones, without the need of getting a PhD in specific tooling :)\\n\\nWe\'ve decided to **experiment with [io_uring](https://unixism.net/loti/what_is_io_uring.html) to maximize the I/O performance** (and at the same time vastly reduce the need of context switches), and at the same time utilize **thread-per-core architecture**, where **each thread is pinned to the CPU core**, thus keeping the data locally, without the need of sharing it with the other threads (share-nothing). In order to achieve this, we\'ve picked up [monoio](https://github.com/bytedance/monoio) runtime, and have already managed (as a starting point for future integration) to fully rewrite existing Tokio runtime into monoio on [this branch](https://github.com/apache/iggy/tree/io_uring_monoio_runtime).\\n\\nAnd just recently, we\'ve established yet another [sandbox repository](https://github.com/iggy-rs/iggy-thread-per-core-sandbox) to tackle the different challenges before deciding on the best solution possible and merging these changes into core Iggy streaming server.\\n\\n![image](/one-year-of-building-the-message-streaming/iggy_tpc.png)\\n\\nSo far, we\'ve got a very simple prototype in place, but there\'s still lots to be done, especially when thinking of:\\n\\n- How to evenly split partitions (the unit of data parallelism) between multiple cores?\\n- How to efficiently rewrite existing server metadata heavily relying on synchronized data with `Arc<>` and `RwLock<>`?\\n- Should we load into memory the same server metadata across all the threads (separately from each other) and notify all of them when something changes?\\n- Should there be single or multiple threads handling the incoming TCP connections?\\n- When `Thread #1` receives the request, which has to access the partition from `Thread #2`, should we use an async two-way channel (remember, no explicit locking and data synchronization between the threads) or maybe just send the descriptor using a one-way channel to the second thread to complete the request?\\n- What and when could become a bottleneck in such architecture?\\n\\nThis is just the tip of an iceberg, and we\'ve already started [studying](https://penberg.org/papers/tpc-ancs19.pdf) some of the existing solutions out there, including [Seastar](https://seastar.io) framework. As you can see, this part has to be done before the clustering, as it involves lots of changes not only regarding the disk I/O, but also networking I/O.\\n\\n\\n\\n## Production readiness\\n\\nIs it ready for production deployment? When you look at the versioning, Rust SDK is at 0.4.* and the server is currently at 0.2.*, which may look like a very long way from v1.0.\\n\\nAs mentioned before, **some of our users already experiment with Iggy** \u2014 and simply because of this, we haven\'t really introduced any significant breaking changes (except one regarding data compression, which was handled by the provided data migration feature).\\n\\nWe can\'t guarantee that it will always be like this, but at least for now, we do not see anything that would dramatically impact the existing solution. One of such things could be the **redesigned storage, I/O, and clustering feature** (as described in the previous paragraph), but even then, we\'ll do our best to make it as seamless upgrade as possible \u2014 and once we achieve that, it means, we\'re getting very close to the version 1.0.\\n\\nFor the time being, if you\'re fine with a **single-node solution that delivers a really good message streaming performance**, give Iggy a try, or at least run the benchmarks to see what are its possibilities and please share your results and thoughts on our [Discord](https://discord.gg/C5Sux5NcRa) \u2014 **your opinion is really important to us and we respect it no matter what**.\\n\\nIt\'s been a very productive year for our core team (we\'ve been and still are doing this in our free time), and once again, huge thanks to all our supporters and contributors!\\n\\n**We\'ve got the fundamentals right, now it\'s high time to make Iggy blazingly fast!**\\n\\n![image](/one-year-of-building-the-message-streaming/iggy_rocket.png)"},{"id":"/2023/12/29/building-message-streaming-in-rust","metadata":{"permalink":"/blogs/2023/12/29/building-message-streaming-in-rust","source":"@site/blog/2023-12-29-building-message-streaming-in-rust.md","title":"Iggy.rs - building message streaming in Rust","description":"Origins","date":"2023-12-29T00:00:00.000Z","tags":[],"readingTime":12.345,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Iggy.rs - building message streaming in Rust","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":[],"hide_table_of_contents":false,"date":"2023-12-29T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Iggy.rs \u2014 one year of building the message streaming","permalink":"/blogs/2024/05/29/one-year-of-building-the-message-streaming"},"nextItem":{"title":"Iggy 0.1.0 release","permalink":"/blogs/iggy-0-1-0-release"}},"content":"## Origins\\n\\nOver half a year ago (in April, to be exact), I eventually decided to learn Rust for good. My previous attempt during the [2022 AoC](https://github.com/spetz/advent-of-code-2022) had failed rather quickly, after a few days of completing the exercises - I finally realized that **I needed a real project to work on**. For the last few years, I\'ve been dealing with the different kinds of distributed systems (mostly using C#), including the typical microservices architecture or Web3. Regardless of their nature, some sort of the messaging between the independent components was always required. I had a chance to use the tools such as [RabbitMQ](https://rabbitmq.com), [ZeroMQ](https://zeromq.org), [Kafka](https://kafka.apache.org) or [Aeron](https://aeron.io) (just to name a few), as well as implementing the low-level peer-to-peer communication [protocols](https://libp2p.io).\\n\\n\x3c!--truncate--\x3e\\n\\nAfter a few months of trying to figure out (or just staying in the limbo I guess), what would be the best project to work on, I decided to build the **message streaming platform** (keep in mind that streaming is [not the same](https://blog.iron.io/message-queue-vs-streaming/) as regular message broker). The other reason (besides getting to know Rust) was to **truly understand the internals of the messaging systems** and the trade-offs that were made by their developers - some of them being the sole implication of the theory of distributed systems (ordering, consistency, partitioning etc.), while others the result of the implementation details (programming language, OS, hardware and so on).\\n\\nAnd this is how the [Iggy.rs](https://iggy.rs) was born. The name is an abbreviation of the Italian Greyhound (yes, I own [two of them](https://www.instagram.com/fabio.and.cookie/)), small yet extremely fast dogs, the best in their class.\\n\\n![image](/building-message-streaming/iggys.jpeg)\\n\\nTherefore, what I want, or actually **what we want** (since there\'s a few of us working on it already) for Iggy.rs to be - the best message streaming platform in its class. **Lightweight** in terms of the resource consumption, **fast** (and predictable) when it comes to the throughput and latency, and **easy to use** when speaking of its API, SDK and configuration of the project.\\n\\n## Project\\n\\nAt the very beginning, Iggy had rather limited functionality, and everything was handled using the [QUIC](https://www.chromium.org/quic/) protocol based on [Quinn](https://github.com/quinn-rs/quinn) library. You could connect multiple applications into the server, and start exchanging the messages between them, simply by appending the data to the stream (from the producer perspective), and fetching the records on the consumer side, by providing an offset (numeric value specifying from which element in the stream, you\'d like to query the data) - that\'s pretty much the very basics of how the message streaming platform works in terms of the underlying infrastructure.\\n\\nAfter spending a few weeks on building the initial version, and then another few weeks on rewriting its core part (yes, prototyping + validation repeated in a continuous loop worked quite well), I managed to implement the persistent streaming server being capable of parallel writes/reads to/from independent streams supporting many distinct apps connected into it. Simply put, one could easily have many applications, and even **thousands of the streams** (depending on how do you decide to split your data between them e.g. one stream for user related events, another one for the payments events etc.) and start producing & consuming the messages without interfering to each other.\\n\\nOn top of this, the support for **TCP** and **HTTP** protocols have been added. Under the hood, the typical architecture of streams, consisting of the topics being split into the partitions, which eventually operate on a raw file data using so-called segments has been implemented as well.\\n\\n![image](/building-message-streaming/iggy_sample.jpeg)\\n\\n**It was one of the \\"aha\\" moments**, when reimplementing the parallel access to the data with the usage of underlying synchronization mechanism (*RwLock* etc.), optimized data structures e.g. for dealing with [bytes](https://crates.io/crates/bytes), along with the [Tokio](https://tokio.rs) **work stealing** approach, yielded the great improvements for the overall throughput.\\n\\nI do believe, that somewhere at this point I had realized, that **Iggy might actually become something useful** - not just a toy project, to be abandoned after reaching its initial goal (which was sort of already achieved).\\n\\n```rust\\nlet polled_messages = client.poll_messages(&PollMessages {\\n  stream_id: Identifier::numeric(1)?,\\n  topic_id: Identifier::named(\\"orders\\")?,\\n  consumer: Consumer::group(Identifier::named(\\"payments\\")?),\\n  partition_id: None,\\n  strategy: PollingStrategy::offset(0),\\n  count: 10,\\n  auto_commit: true,\\n}).await?;\\n```\\n\\nAfter running some benchmarks (yes, we have a dedicated app for the **[benchmarking purposes](https://github.com/apache/iggy/tree/master/core/bench)**) and seeing the promising numbers (**range of 2-6 GB/s for both, writes & reads when processing millions of messages**), I eventually decided to give it a long-term shot. Being fully aware that there\'s still lots to be done (speaking of many months, or even years), I couldn\'t be more happy to find out that there\'s also someone else out there, who would like to contribute to the project and become a part of the team.\\n\\n![image](/building-message-streaming/iggy_bench.jpeg)\\n\\n## Team\\n\\nAt the time of writing this post, **Iggy consists of around 10 members** contributing to its different parts. Some of us do work on the core streaming server, while the other ones are focused on SDKs for the different programming languages or tooling such as Web UI or CLI - all these projects are equally important, as they add up to the overall ecosystem. But how do you actually gather a team of open source contributors, who are willing to spend their free time working on it?\\n\\nWell, I wish I had an answer to that question - honestly, in case of Iggy I wasn\'t actually looking for anyone, as I didn\'t think this could be an interesting project to work on (except for myself). Then how did that happen anyway? There were only 2 things in common - all the people that joined the project were part of the same Discord communities, yet more importantly **they all shared the passion for programming**, and I\'m not talking about Rust language specifically. From junior to senior, from embedded to front-end developers - regardless of the years of experience and current occupation, everyone has found a way to implement something meaningful.\\n\\n![image](/building-message-streaming/iggy_server.jpeg)\\n\\nFor example, when I asked one guy what was the reason behind building an SDK in Go, the reply was the need of playing with and learning a new language. Why C# SDK? Well, the other guy wanted to dive more into the low-level concepts and decided to squeeze out great performance from the managed runtime. Why build Web UI in Svelte? At work, I mostly use React, and I wanted to learn a new framework - another member said.\\n\\nMy point is - as long as you believe in what you\'re building, and you\'re consistent about it (it was one of the main reasons why I\'ve been contributing to Iggy every day since its inception, and still doing so), there\'s a chance that someone out there will notice it and happily join you in your efforts. **Lead by example, or whatever you call it.**\\n\\nAt the same time, we\'ve started receiving the first, **external contributions from all around the world** - whether talking about simpler tasks, or the more sophisticated ones, requiring significant amount of time being spent on both, the implementation and the discussions to eventually deliver the code.\\n\\nIt gave us even more confidence, that there are other people (outside our internal bubble), who find this project to be interesting and worth spending their time. And without all these amazing contributors, it\'d be much harder (or even impossible) to deliver so many features.\\n\\n## Features\\n\\nAt first, let me just point out some of the properties and features that are part of the core streaming server:\\n\\n- **Highly performant**, persistent append-only log for the message streaming\\n- **Very high throughput** for both writes and reads\\n- **Low latency and predictable resource usage** thanks to the Rust compiled language (no GC)\\n- **Users authentication and authorization** with granular permissions and PAT (Personal Access Tokens)\\n- Support for multiple streams, topics and partitions\\n- Support for **multiple transport protocols** (QUIC, TCP, HTTP)\\n- Fully operational RESTful API which can be optionally enabled\\n- Available client SDK in multiple languages\\n- **Works directly with the binary data** (lack of enforced schema and serialization/deserialization)\\n- Configurable server features (e.g. caching, segment size, data flush interval, transport protocols etc.)\\n- Possibility of storing the **consumer offsets** on the server\\n- Multiple ways of polling the messages:\\n  - By offset (using the indexes)\\n  - By timestamp (using the time indexes)\\n  - First/Last N messages\\n  - Next N messages for the specific consumer\\n- Possibility of **auto committing the offset** (e.g. to achieve *at-most-once* delivery)\\n- **Consumer groups** providing the message ordering and horizontal scaling across the connected clients\\n- **Message expiry** with auto deletion based on the configurable **retention policy**\\n- Additional features such as **server side message deduplication**\\n- **TLS** support for all transport protocols (TCP, QUIC, HTTPS)\\n- Optional server-side as well as client-side **data encryption** using AES-256-GCM\\n- Optional metadata support in the form of **message headers**\\n- Built-in **CLI** to manage the streaming server\\n- Built-in **benchmarking app** to test the performance\\n- **Single binary deployment** (no external dependencies)\\n- Running as a single node (no cluster support yet)\\n\\nAnd as already mentioned, we\'ve been working on SDKs for the multiple programming languages:\\n\\n- [Rust](https://crates.io/crates/iggy)\\n- [C#](https://github.com/iggy-rs/iggy-dotnet-client)\\n- [Go](https://github.com/iggy-rs/iggy-go-client)\\n- [Node](https://github.com/iggy-rs/iggy-node-client)\\n- [Python](https://github.com/iggy-rs/iggy-python-client)\\n- [Java](https://github.com/iggy-rs/iggy-java-client)\\n\\nPlease keep in mind, though, that some of them e.g. for Rust or C# are more up to date with the recent server changes, while the other ones might still need to do some catching up with the latest features. However, given the amount of available methods on the server\'s API and the underlying TCP/UDP stack with custom serialization to be implemented from the scratch (except for HTTP transport, that\'s the easier one), I\'d say we\'re doing quite ok, and I can\'t stress enough **how grateful I am to all the contributors for their huge amount of work**!\\n\\nBut wait, there\'s even more - what would be a message streaming platform without some additional tooling for managing it? We\'ve also been developing the **[CLI](https://github.com/apache/iggy/tree/master/cmd)**.\\n\\n![image](/building-message-streaming/iggy_cli.jpeg)\\n\\nAs well as modern **[Web UI](https://github.com/iggy-rs/iggy-web-ui)** to make it happen :)\\n\\n![image](/building-message-streaming/iggy_web_ui.jpeg)\\n\\nLast but not least, we\'ve got a fully-featured [CI/CD pipeline](https://github.com/apache/iggy/actions) responsible for running all the checks and tests on multiple platforms, and finally producing the release artifacts and [Docker images](https://hub.docker.com/u/iggyrs).\\n\\n![image](/building-message-streaming/iggy_ci_cd.jpeg)\\n\\nAt first glance, it might look like there\'s plenty of features already in place, but for anyone who has ever worked with the message streaming infrastructure before, that might be just a tip of an iceberg, thus let\'s discuss the roadmap.\\n\\n## Roadmap\\n\\nAfter gaining some traction a few months ago (mostly due to landing on the GitHub trending page in July), we\'ve talked to some users potentially interested in making Iggy part of their infrastructure (there\'s even one [company](https://neferdata.com) using it already), and discussed what features would be a good addition to the current stack.\\n\\n![image](/building-message-streaming/gh_trending.jpeg)\\n\\nConsidering what\'s already there, being worked on or planned for the future releases, such as interactive CLI, modern Web UI, optional data compression and archivization, plugin support or multiple SDKs, there are at least three additional challenges to overcome:\\n\\n**Clustering** - the possibility of having a **highly available and fault tolerant** distributed message streaming platform in a production environment, is typically one of the most important aspects when considering the particular tool. While it wouldn\'t be too difficult to implement the extension (think of a simple proxy/load balancer), allowing to monitor and deliver the data either to the primary or secondary replica (treated as a fallback server) and switch between them when one of the nodes goes down, such a solution would still result in SPOF and wouldn\'t really scale. Instead, we\'ve started experimenting with [Raft](https://raft.github.io/) consensus mechanism (de facto the industry standard) in a dedicated [repository](https://github.com/iggy-rs/iggy-cluster-sandbox), which should allow us in delivering the truly fault tolerant, distributed infrastructure with an additional data replication at the partition level (so-called unit of parallelization).\\n\\n![image](/building-message-streaming/leader_elections.jpeg)\\n\\n**Low-level I/O** - although the current results (based on the benchmarking tool measuring the throughput etc.) are satisfying, we strongly believe that there\'s still (potentially a huge) room for improvement. We\'re planning to use **[io_uring](https://unixism.net/loti/what_is_io_uring.html)** for all I/O operations (disk or network related). The brand new, completion based API (available in the recent Linux kernels) shows a significant boost when compared to the existing solutions such as **epoll** or **kqueue** - at the end of the day, the streaming server at its core is all about writing & reading data to/from disk and sending it via the network buffer. We\'ve decided to give a try **[monoio](https://github.com/bytedance/monoio)** runtime, as it seems to be the most performant one. Going further, we\'d like to incorporate techniques such as **zero-copy**, **kernel bypass** and all the other goodies e.g. from **[DPDK](https://www.dpdk.org/)** or other relevant frameworks.\\n\\n**Thread-per-core** - in order to avoid the rather costly context switches due to the usage of synchronization mechanism when accessing the data from the different threads (e.g. via Tokio\'s work stealing mechanism), we\'re planning to explore (or actually, already doing it, in the previously mentioned repository for clustering sandbox) thread-per-core architecture, once again, delivered as part of **[monoio](https://github.com/bytedance/monoio)** runtime. The overall idea can be described in two words - **share nothing** (or as little as possible). For example, the streams could be tied to the particular CPU cores, resulting in **no additional overhead** (via *Mutexes*, *RwLocks* etc.) when writing or reading the data. As good as it might sound, there are always some tradeoffs - what if some specific streams are more frequently accessed than the others? Would the remaining cores remain idle instead of doing something useful? On the other hand, tools such as [ScyllaDB](https://www.scylladb.com/product/technology/shard-per-core-architecture/) or [Redpanda](https://redpanda.com/blog/tpc-buffers) seem to be leveraging this model quite effectively (both are using the same [Seastar](https://seastar.io/) framework). We will be looking for the answers, before deciding which approach (thread-per-core or work stealing) suits Iggy better in the future.\\n\\n## Future\\n\\n**Why building another message streaming then?** A few months ago, I would probably answer - strictly for fun. Yet, after exploring more in-depth the status quo, what we would like to achieve is sort of twofold - on one hand, it\'d be great to deliver the general-purpose tool, such as Kafka. On the other hand, why not to try and really push hard the OS and hardware to its limits when speaking of the performance, reliability, throughput and latency, something what e.g. Aeron does? And what if we could **put this all together** into the easy-to-use, unified platform, supporting the most popular programming languages, with the addition of modern CLI and Web UI for managing it?\\n\\nOnly the time will tell, but **we\'re already excited enough to challenge ourselves**. We\'d love to hear your thoughts, ideas and **feedback** - anything that will help us in building the best message streaming platform in Rust that you will enjoy using! Feel free to join our [Discord](https://discord.gg/C5Sux5NcRa) community and let us know what do you think :)\\n\\nP.S.\\n\\nThis blog uses [Rust](https://www.getzola.org/)."},{"id":"iggy-0-1-0-release","metadata":{"permalink":"/blogs/iggy-0-1-0-release","source":"@site/blog/2023-11-26-release-0.1.0.md","title":"Iggy 0.1.0 release","description":"We are happy to announce that Iggy.rs has reached the 0.1.0 release. This is a major milestone for the project, as it\'s getting closer to the first stable release.","date":"2023-11-26T00:00:00.000Z","tags":[{"inline":true,"label":"release","permalink":"/blogs/tags/release"}],"readingTime":1.88,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Iggy 0.1.0 release","slug":"iggy-0-1-0-release","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":["release"],"hide_table_of_contents":false,"date":"2023-11-26T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Iggy.rs - building message streaming in Rust","permalink":"/blogs/2023/12/29/building-message-streaming-in-rust"},"nextItem":{"title":"Personal access tokens","permalink":"/blogs/personal-access-tokens"}},"content":"We are happy to announce that Iggy.rs has reached the `0.1.0` release. This is a major milestone for the project, as it\'s getting closer to the first stable release.\\n\\n\x3c!--truncate--\x3e\\n\\n![0.1.0](/img/iggy_0_1_0.jpg)\\n\\n## Release notes\\n\\nThe **streaming server**, being the core of the project, has been improved in many ways. Some of the most important features and bug fixes have to do with the area of memory management, such as keeping the configurable size of the cache under control, extended security including the authentication and authorization, added personal access tokens, and the overall stability of the server.\\n\\n![CI](/img/iggy_server_0_1_0.jpeg)\\n\\nAll the core components (server + SDK + CLI + bench) have to go through a lot of automated tests on the different OS, which are run on every commit, and the code coverage is constantly increasing. We\'ve put a lot of offert into making our CI/CD pipeline as robust as possible, and we are happy with the results. \\n\\n![CI](/img/iggy_ci_0_1_0.jpeg)\\n\\nOn top of it, the built-in benchmarking application has been significantly improved and now has its own CLI tool, which makes it easier to use.\\n\\n![Bench](/img/iggy_bench_0_1_0.jpeg)\\n\\nAdditionally, we\'ve also a great **CLI** being actively developed, which purpose is to allow the users to interact with the server using the command line.\\n\\n![CLI](/img/iggy_cli_0_1_0.jpeg)\\n\\nAnd then there\'s a modern, responsive, and fast **Web UI**, which provides a way to interact with the server using the browser.\\n\\n![Web UI](/img/iggy_web_ui_0_1_0.jpeg)\\n\\nWhile Rust **SDK** is the most mature one, and the C# is a close second, the remaining ones for Java, Go, Python, and NodeJS are catching up. It\'s not an easy task to keep all of them in sync, but we are doing our best to make sure that the SDKs are up-to-date with the latest changes in the server.\\n\\nLast but not least, we are **extremely grateful to the community**, which is growing every day. We are getting more and more feedback, and we are doing our best to address all the issues and feature requests. We are also getting more and more contributions, which is a great sign that the project is going in the right direction.\\n\\nCome and join our [Discord](https://discord.gg/C5Sux5NcRa) server, and let us know what you think about the project. We are looking forward to hearing from you!"},{"id":"personal-access-tokens","metadata":{"permalink":"/blogs/personal-access-tokens","source":"@site/blog/2023-10-12-personal-access-tokens.md","title":"Personal access tokens","description":"Since the most recent update, Iggy.rs supports personal access tokens, which can be used to authenticate the clients, instead of the username and password. The tokens can be created and deleted using the available APIs.","date":"2023-10-12T00:00:00.000Z","tags":[{"inline":true,"label":"new-features","permalink":"/blogs/tags/new-features"},{"inline":true,"label":"personal-access-tokens","permalink":"/blogs/tags/personal-access-tokens"},{"inline":true,"label":"pat","permalink":"/blogs/tags/pat"}],"readingTime":2.35,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Personal access tokens","slug":"personal-access-tokens","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":["new-features","personal-access-tokens","pat"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Iggy 0.1.0 release","permalink":"/blogs/iggy-0-1-0-release"},"nextItem":{"title":"Consumer identifier","permalink":"/blogs/consumer-identifier"}},"content":"Since the most recent update, Iggy.rs supports personal access tokens, which can be used to authenticate the clients, instead of the username and password. The tokens can be created and deleted using the available APIs.\\n\\n\x3c!--truncate--\x3e\\n\\n## Breaking changes\\n\\nNo breaking changes have been introduced in neither Iggy server, nor Iggy SDK. The server does support the new `PAT` authentication method since version `0.0.40`, and the SDK since version `0.0.100`. The initial changes are part of the commit [#e74c25e](https://github.com/apache/iggy/commit/e74c25e058b1f39119ee89b5ada5d93f171cb221).\\n\\n## Personal access tokens\\n\\nPAT is a simple idea, which allows authenticating the clients using a token, instead of the regular credentials (username and password). This approach might feel safer for some users, as the token can be deleted at any time, and it\'s not tied to the user\'s password. The tokens can be created, listed and deleted using the available APIs. PAT has an optional expiry, which can be set when creating the token.\\n\\n\\n```rust\\npub struct CreatePersonalAccessToken {\\n    pub name: String,\\n    pub expiry: Option<u32>,\\n}\\n\\npub struct DeletePersonalAccessToken {\\n    pub name: String,\\n}\\n\\npub struct GetPersonalAccessTokens {}\\n\\npub struct LoginWithPersonalAccessToken {\\n    pub token: String,\\n}\\n```\\n\\nEach token must have a unique `name`, which is used to identify the token. The `expiry` field is optional, and if set, it must be the amount of seconds since the token creation on the server side, after which the token will expire. A single user may create the maximum of 100 tokens.\\n\\n```rust\\nasync fn get_personal_access_tokens(\\n    &self,\\n    command: &GetPersonalAccessTokens,\\n) -> Result<Vec<PersonalAccessTokenInfo>, Error>;\\n\\nasync fn create_personal_access_token(\\n    &self,\\n    command: &CreatePersonalAccessToken,\\n) -> Result<RawPersonalAccessToken, Error>;\\n\\nasync fn delete_personal_access_token(\\n    &self,\\n    command: &DeletePersonalAccessToken,\\n) -> Result<(), Error>;\\n\\nasync fn login_with_personal_access_token(\\n    &self,\\n    command: &LoginWithPersonalAccessToken,\\n) -> Result<IdentityInfo, Error>;\\n```\\n\\nWhen creating the token, the server returns the `RawPersonalAccessToken` struct, which contains the string `token` value.\\n\\n```rust\\npub struct RawPersonalAccessToken {\\n    pub token: String,\\n}\\n```\\n\\nThis is returned **only once**, as it\'s a raw, secure token, which should be stored by the client. The server only stores the hashed version of the token, and it\'s not possible to retrieve the original value. The token can be used to authenticate the client using the `LoginWithPersonalAccessToken` command.\\n\\nThe following structure is returned when listing the tokens. It contains the `name` and an optional `expiry` fields. Please note that the `expiry` is the actual timestamp (Epoch in microseconds) of when the token will expire, and not the amount of seconds since the token creation (like provided in the `CreatePersonalAccessToken` command).\\n\\n```rust\\npub struct PersonalAccessTokenInfo {\\n    pub name: String,\\n    pub expiry: Option<u64>,\\n}\\n```\\n\\nLogin with PAT is very similar to the regular login with user credentials, and it returns the same `IdentityInfo` structure.\\n\\n## Configuration\\n\\nOn the server side, you can enable the background task responsible for deleting the expired tokens.\\n\\n```toml\\n[personal_access_token_cleaner]\\nenabled = true\\ninterval = 60\\n```"},{"id":"consumer-identifier","metadata":{"permalink":"/blogs/consumer-identifier","source":"@site/blog/2023-10-02-consumer-identifier.md","title":"Consumer identifier","description":"In the latest update, the Iggy server as well as the clients for all the available transport protocols have been extended with the support for consumer identifier. Whether you poll the messages, store the consumer offsets, or create consumer groups, you can use the well-established identifier type, instead of just u32, which is now a common standard for the resources\' identification such as streams, topics, users and consumers.","date":"2023-10-02T00:00:00.000Z","tags":[{"inline":true,"label":"new-features","permalink":"/blogs/tags/new-features"},{"inline":true,"label":"consumers","permalink":"/blogs/tags/consumers"},{"inline":true,"label":"identifier","permalink":"/blogs/tags/identifier"}],"readingTime":1.955,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Consumer identifier","slug":"consumer-identifier","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":["new-features","consumers","identifier"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Personal access tokens","permalink":"/blogs/personal-access-tokens"},"nextItem":{"title":"Users and permissions","permalink":"/blogs/users-and-permissions"}},"content":"In the latest update, the Iggy server as well as the clients for all the available transport protocols have been extended with the support for **consumer identifier**. Whether you poll the messages, store the consumer offsets, or create consumer groups, you can use the well-established `identifier` type, instead of just `u32`, which is now a common standard for the resources\' identification such as streams, topics, users and consumers.\\n\\n\x3c!--truncate--\x3e\\n\\n## Breaking changes\\n\\nThe breaking changes have been introduced in Iggy server, starting from version `0.0.30` and Iggy SDK, starting from version `0.0.90`. The final commit containing all the mentioned changes is [#655f9b6](https://github.com/apache/iggy/commit/655f9b6bccb0ae4148422f32475a9bedc09827d2).\\n\\n## Consumer identifier\\n\\n```rust\\npub struct Consumer {\\n    pub kind: ConsumerKind,\\n    pub id: Identifier,\\n}\\n```\\n\\nAll the commands, such as `PollMessages`, `StoreConsumerOffset` and `GetConsumerOffset` which contain the `consumer` field, now require an update to the underlying serialization, as the `id` field is now an `Identifier` type, instead of `u32`. The serialization is the same, as when working with the `stream`, `topic` or `user` resources.\\n\\nFrom now on, whenever working with the `consumer` or `consumer group` resource, you can use the numeric or text identifier.\\n\\n## Consumer groups\\n\\n```rust\\npub struct CreateConsumerGroup {\\n    pub stream_id: Identifier,\\n    pub topic_id: Identifier,\\n    pub consumer_group_id: u32,\\n    pub name: String,\\n}\\n```\\n\\nWhen creating the consumer group, the required `name` field has been added (must be **unique per topic**), and is serialized as usual, at the end of the payload, starting with the `name length` (4 bytes) and the `name` itself (N bytes).\\n\\n\\n```rust\\npub struct DeleteConsumerGroup {\\n    pub stream_id: Identifier,\\n    pub topic_id: Identifier,\\n    pub consumer_group_id: Identifier,\\n}\\n\\npub struct GetConsumerGroup {\\n    pub stream_id: Identifier,\\n    pub topic_id: Identifier,\\n    pub consumer_group_id: Identifier,\\n}\\n\\npub struct JoinConsumerGroup {\\n    pub stream_id: Identifier,\\n    pub topic_id: Identifier,\\n    pub consumer_group_id: Identifier,\\n}\\n\\npub struct LeaveConsumerGroup {\\n    pub stream_id: Identifier,\\n    pub topic_id: Identifier,\\n    pub consumer_group_id: Identifier,\\n}\\n```\\n\\nThe `DeleteConsumerGroup`, `GetConsumerGroup`, `JoinConsumerGroup` and `LeaveConsumerGroup` commands have been extended with the `consumer_group_id` field, which is now an `Identifier` type, instead of `u32`.\\n\\n\\n```rust\\npub struct ConsumerGroup {\\n    pub id: u32,\\n    pub name: String,\\n    pub partitions_count: u32,\\n    pub members_count: u32,\\n}\\n\\npub struct ConsumerGroupDetails {\\n    pub id: u32,\\n    pub name: String,\\n    pub partitions_count: u32,\\n    pub members_count: u32,\\n    pub members: Vec<ConsumerGroupMember>,\\n}\\n```\\n\\nWhen fetching the consumer group(s), the `ConsumerGroup` and `ConsumerGroupDetails` structs have been extended with the `name` field, which serialized after `members_count`."},{"id":"users-and-permissions","metadata":{"permalink":"/blogs/users-and-permissions","source":"@site/blog/2023-09-20-users-and-permissions.md","title":"Users and permissions","description":"In the most recent update, the Iggy server as well as the clients for all the available transport protocols have been extended with the support for users and permissions. From now on, you can additionally secure your data using the authentication and authorization by specifying the granular set of permissions for each user.","date":"2023-09-20T00:00:00.000Z","tags":[{"inline":true,"label":"new-features","permalink":"/blogs/tags/new-features"},{"inline":true,"label":"users","permalink":"/blogs/tags/users"},{"inline":true,"label":"permissions","permalink":"/blogs/tags/permissions"}],"readingTime":6.24,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Users and permissions","slug":"users-and-permissions","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":["new-features","users","permissions"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Consumer identifier","permalink":"/blogs/consumer-identifier"},"nextItem":{"title":"Updated schemas","permalink":"/blogs/updated-schemas"}},"content":"In the most recent update, the Iggy server as well as the clients for all the available transport protocols have been extended with the support for **users and permissions**. From now on, you can additionally secure your data using the authentication and authorization by specifying the granular set of permissions for each user.\\n\\n\x3c!--truncate--\x3e\\n\\n## Breaking changes\\n\\nIn general, there are no breaking changes, except a new field added to the `ClientInfo` and `ClientInfoDetails` structs returned by `get_client()` and `get_clients()` methods.\\n\\n```rust\\npub struct ClientInfo {\\n    pub client_id: u32,\\n    pub user_id: Option<u32>, // New optional field\\n    pub address: String,\\n    pub transport: String,\\n    pub consumer_groups_count: u32,\\n}\\n```\\n\\nThe `user_id` field is optional, and it\'s only returned when the user is authenticated. Otherwise, it\'s set to `None` (0 value for binary transport) or `null` (HTTP transport).\\n\\nKeep in mind that the `client` represents the application connected to the server - since it\'s possible that multiple applications would share the same user credentials, thus you can think of a `client` as a single connection from the application to the server (1 user -> N clients).\\n\\nThis breaking change has been introduced with the commit [#23b3309](https://github.com/apache/iggy/commit/23b330928aa38463ef907c95a9f672fa7b728881). The available [iggy crate](https://crates.io/crates/iggy) supports these changes since version 0.0.80.\\n\\n## Configuration\\n\\nCurrently, the authentication and authorization can be enabled in the `server.toml` configuration via `[system.user]` section:\\n\\n```toml\\n[system.user]\\nauthentication_enabled = true\\nauthorization_enabled = true\\n```\\n\\nThis might change in the future releases (e.g. authentication and authorization will always be enabled), but in order to avoid the breaking changes to the existing clients SDKs, these options are configurable for now.\\n\\nThe additional settings for HTTP API which uses JWT (JSON Web Token), can be found in the `[http.jwt]` section:\\n\\n```toml\\n[http.jwt]\\nalgorithm = \\"HS256\\"\\naudience = \\"iggy.rs\\"\\nexpiry = 3600\\nencoding_secret = \\"top_secret$iggy.rs$_jwt_HS256_key#!\\"\\ndecoding_secret = \\"top_secret$iggy.rs$_jwt_HS256_key#!\\"\\nuse_base64_secret = false\\n```\\n\\n## Authentication and authorization\\n\\nThe overall implementation of the users authentication and authorization is rather typical. All the server methods except `ping` and `login` require the user to be authenticated. The `login` method is used to authenticate the user, and it returns the optional `token` which is then used to authenticate the user in the subsequent requests. For now, the `token` is only used by HTTP API based on JWT, for the binary transport there\'s no token - since, it\'s a stateful protocol, the server keeps track of the authenticated connections.\\n\\nThe authorization is based on the permissions, which are defined for each user. All the server methods have the specific authorization policies applied. For example, you can allow the user to read all the streams, or manage all the topics, while on the other hand, you could also specify only the set of particular streams or topics to which the user can send and/or poll the message to/from.\\n\\nTake a look at the following JSON example to get the overall idea:\\n\\n```json\\n{\\n  \\"permissions\\": {\\n    \\"global\\": {\\n      \\"manage_servers\\": false,\\n      \\"read_servers\\": true,\\n      \\"manage_users\\": true,\\n      \\"read_users\\": true,\\n      \\"manage_streams\\": false,\\n      \\"read_streams\\": true,\\n      \\"manage_topics\\": false,\\n      \\"read_topics\\": true,\\n      \\"poll_messages\\": true,\\n      \\"send_messages\\": true\\n    },\\n    \\"streams\\": {\\n      \\"1\\": {\\n        \\"manage_stream\\": false,\\n        \\"read_stream\\": true,\\n        \\"manage_topics\\": false,\\n        \\"read_topics\\": true,\\n        \\"poll_messages\\": true,\\n        \\"send_messages\\": true,\\n        \\"topics\\": {\\n          \\"1\\": {\\n            \\"manage_topic\\": false,\\n            \\"read_topic\\": true,\\n            \\"poll_messages\\": true,\\n            \\"send_messages\\": true\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\nEach user might have the global permissions which do apply to all the available streams and topics, as well as the granular permissions for each stream and its topics (this is a simple hashmap using the stream or topic ID as its key).\\n\\nFor example, the `root user`, which has the default username and password `iggy` and cannot be updated or deleted, can do everything, simply by having the following permissions:\\n\\n```json\\n{\\n  \\"permissions\\": {\\n    \\"global\\": {\\n      \\"manage_servers\\": true,\\n      \\"read_servers\\": true,\\n      \\"manage_users\\": true,\\n      \\"read_users\\": true,\\n      \\"manage_streams\\": true,\\n      \\"read_streams\\": true,\\n      \\"manage_topics\\": true,\\n      \\"read_topics\\": true,\\n      \\"poll_messages\\": true,\\n      \\"send_messages\\": true\\n    },\\n    \\"streams\\": null\\n  }\\n}\\n```\\n\\n## SDK\\n\\nThe new `UserClient` trait has been added in order to support the users and permissions feature. It is implemented for all the available transport protocols.\\n\\n```rust\\npub trait UserClient {\\n    async fn get_user(&self, command: &GetUser) -> Result<UserInfoDetails, Error>;\\n    async fn get_users(&self, command: &GetUsers) -> Result<Vec<UserInfo>, Error>;\\n    async fn create_user(&self, command: &CreateUser) -> Result<(), Error>;\\n    async fn delete_user(&self, command: &DeleteUser) -> Result<(), Error>;\\n    async fn update_user(&self, command: &UpdateUser) -> Result<(), Error>;\\n    async fn update_permissions(&self, command: &UpdatePermissions) -> Result<(), Error>;\\n    async fn change_password(&self, command: &ChangePassword) -> Result<(), Error>;\\n    async fn login_user(&self, command: &LoginUser) -> Result<IdentityInfo, Error>;\\n    async fn logout_user(&self, command: &LogoutUser) -> Result<(), Error>;\\n}\\n```\\n\\n## Commands and models\\n\\nThe following commands and response models have been added:\\n\\n```rust\\npub struct IdentityInfo {\\n    pub user_id: u32,\\n    pub token: Option<String>,\\n}\\n```\\n\\n```rust\\npub struct UserInfo {\\n    pub id: u32,\\n    pub created_at: u64,\\n    pub status: UserStatus,\\n    pub username: String,\\n}\\n\\npub struct UserInfoDetails {\\n    pub id: u32,\\n    pub created_at: u64,\\n    pub status: UserStatus,\\n    pub username: String,\\n    pub permissions: Option<Permissions>,\\n}\\n```\\n\\n```rust\\npub struct Permissions {\\n    pub global: GlobalPermissions,\\n    pub streams: Option<HashMap<u32, StreamPermissions>>,\\n}\\n\\npub struct GlobalPermissions {\\n    pub manage_servers: bool,\\n    pub read_servers: bool,\\n    pub manage_users: bool,\\n    pub read_users: bool,\\n    pub manage_streams: bool,\\n    pub read_streams: bool,\\n    pub manage_topics: bool,\\n    pub read_topics: bool,\\n    pub poll_messages: bool,\\n    pub send_messages: bool,\\n}\\n\\npub struct StreamPermissions {\\n    pub manage_stream: bool,\\n    pub read_stream: bool,\\n    pub manage_topics: bool,\\n    pub read_topics: bool,\\n    pub poll_messages: bool,\\n    pub send_messages: bool,\\n    pub topics: Option<HashMap<u32, TopicPermissions>>,\\n}\\n\\npub struct TopicPermissions {\\n    pub manage_topic: bool,\\n    pub read_topic: bool,\\n    pub poll_messages: bool,\\n    pub send_messages: bool,\\n}\\n```\\n\\n```rust\\npub struct GetUser {\\n    pub user_id: Identifier,\\n}\\n```\\n\\n```rust\\npub struct GetUsers {}\\n```\\n\\n```rust\\npub struct CreateUser {\\n    pub username: String,\\n    pub password: String,\\n    pub status: UserStatus,\\n    pub permissions: Option<Permissions>,\\n}\\n```\\n\\n```rust\\npub struct DeleteUser {\\n    pub user_id: Identifier,\\n}\\n```\\n\\n```rust\\npub struct UpdateUser {\\n    pub user_id: Identifier,\\n    pub username: Option<String>,\\n    pub status: Option<UserStatus>,\\n}\\n```\\n\\n```rust\\npub struct UpdatePermissions {\\n    pub user_id: Identifier,\\n    pub permissions: Option<Permissions>,\\n}\\n```\\n\\n```rust\\npub struct ChangePassword {\\n    pub user_id: Identifier,\\n    pub current_password: String,\\n    pub new_password: String,\\n}\\n```\\n\\n```rust\\npub struct LoginUser {\\n    pub username: String,\\n    pub password: String,\\n}\\n```\\n\\n```rust\\npub struct LogoutUser {}\\n```\\n\\nSimilar to the stream and topic ID, the user will always have the unique numeric ID and the username. The username must be within the range of 3\u201350 chars and will always be lowercased, using the same regex as the stream and topic name `^[\\\\w\\\\.\\\\-\\\\s]+$`. The password must be within the range of 3\u2013100 chars and will be hashed using the bcrypt algorithm.\\n\\n## HTTP API\\n\\nThe following endpoints have been added, and as always, you can find them in the `server.http` file in the main repository.\\n\\n```\\nPOST {{url}}/users/login\\nContent-Type: application/json\\n\\n{\\n  \\"username\\": \\"iggy\\",\\n  \\"password\\": \\"iggy\\"\\n}\\n\\nPOST {{url}}/users/logout\\nAuthorization: Bearer {{token}}\\nContent-Type: application/json\\n\\n{\\n}\\n\\nPOST {{url}}/users\\nAuthorization: Bearer {{token}}\\nContent-Type: application/json\\n\\n{\\n  \\"username\\": \\"user1\\",\\n  \\"password\\": \\"secret\\",\\n  \\"status\\": \\"active\\",\\n  \\"permissions\\": null\\n}\\n\\nGET {{url}}/users\\nAuthorization: Bearer {{token}}\\n\\nGET {{url}}/users/user1\\nAuthorization: Bearer {{token}}\\n\\nPUT {{url}}/users/user1\\nAuthorization: Bearer {{token}}\\nContent-Type: application/json\\n\\n{\\n  \\"username\\": \\"user1,\\n  \\"status\\": \\"active\\",\\n  \\"permissions\\": null\\n}\\n\\nPUT {{url}}/users/user1/password\\nAuthorization: Bearer {{token}}\\nContent-Type: application/json\\n\\n{\\n  \\"current_password\\": \\"secret\\",\\n  \\"new_password\\": \\"secret1\\"\\n}\\n\\nPUT {{url}}/users/user1permissions\\nAuthorization: Bearer {{token}}\\nContent-Type: application/json\\n\\n{\\n  \\"permissions\\": {\\n    \\"global\\": {\\n      \\"manage_servers\\": false,\\n      \\"read_servers\\": true,\\n      \\"manage_users\\": true,\\n      \\"read_users\\": true,\\n      \\"manage_streams\\": false,\\n      \\"read_streams\\": true,\\n      \\"manage_topics\\": false,\\n      \\"read_topics\\": true,\\n      \\"poll_messages\\": true,\\n      \\"send_messages\\": true\\n    },\\n    \\"streams\\": {\\n      \\"1\\": {\\n        \\"manage_stream\\": false,\\n        \\"read_stream\\": true,\\n        \\"manage_topics\\": false,\\n        \\"read_topics\\": true,\\n        \\"poll_messages\\": true,\\n        \\"send_messages\\": true,\\n        \\"topics\\": {\\n          \\"1\\": {\\n            \\"manage_topic\\": false,\\n            \\"read_topic\\": true,\\n            \\"poll_messages\\": true,\\n            \\"send_messages\\": true\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\n\\nDELETE {{url}}/users/user1\\nAuthorization: Bearer {{token}}\\n```"},{"id":"updated-schemas","metadata":{"permalink":"/blogs/updated-schemas","source":"@site/blog/2023-08-31-updated-schemas.md","title":"Updated schemas","description":"The latest update introduces the changes to the PollMessages and GetConsumerOffset commands response schema, as well as the Stream, Topic and Partition structs extended with created_at field.","date":"2023-08-31T00:00:00.000Z","tags":[{"inline":true,"label":"update","permalink":"/blogs/tags/update"},{"inline":true,"label":"schema","permalink":"/blogs/tags/schema"}],"readingTime":1.51,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Updated schemas","slug":"updated-schemas","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":["update","schema"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Users and permissions","permalink":"/blogs/users-and-permissions"},"nextItem":{"title":"Message expiry","permalink":"/blogs/message-expiry"}},"content":"The latest update introduces the changes to the `PollMessages` and `GetConsumerOffset` commands response schema, as well as the `Stream`, `Topic` and `Partition` structs extended with `created_at` field.\\n\\n\x3c!--truncate--\x3e\\n\\n## Breaking changes\\n\\nBreaking changes have been introduced with the commit [#670a8ba](https://github.com/apache/iggy/commit/670a8baba8617bb7e52bd51d556a621349e2e9f0). The available [iggy crate](https://crates.io/crates/iggy) supports these changes since version 0.0.60.\\n\\n## PollMessages\\n\\nBy default, the `PollMessages` used to return `Vec<Message>` as a response, which has been changed to `PolledMessages` struct, that contains the `partition_id`, `current_offset` and `messages` field. The reason for this change was to provide more information about the current offset, and the partition from which the messages have been fetched - this might be especially useful when using the consumer groups feature, where the partition is calculated on the server-side.\\n\\n```rust\\npub struct PolledMessages {\\n    pub partition_id: u32,\\n    pub current_offset: u64,\\n    pub messages: Vec<Message>,\\n}\\n```\\n\\nSerialization:\\n\\n```\\nPartition ID (4 bytes) + Current offset (8 bytes) + Messages count (4 bytes) + Messages (N bytes)\\n```\\n\\n## GetConsumerOffset\\n\\nPreviously, the `GetConsumerOffset` command used to return `u64` as a response, which has been changed to `ConsumerOffsetInfo` struct, that contains the `partition_id`, `current_offset` and `stored_offset` field. Similar to the `PollMessages` response update, the additional data might be helpful when using the consumer groups feature.\\n\\n```rust\\npub struct ConsumerOffsetInfo {\\n    pub partition_id: u32,\\n    pub current_offset: u64,\\n    pub stored_offset: u64,\\n}\\n```\\n\\nSerialization:\\n\\n```\\nPartition ID (4 bytes) + Current offset (8 bytes) + Stored offset (8 bytes)\\n```\\n\\n## Stream, Topic, Partition\\n\\nThe `Stream`, `Topic` and `Partition` structs (along with the additional `Details` models when fetching the single object, not a list) have been extended with the `created_at` field, which contains the timestamp of the creation time (EPOCH in microseconds).\\n\\nThe `created_at` field which is `u64` (8 bytes) is always serialized right after the `id` field, and then the remaining fields follow."},{"id":"message-expiry","metadata":{"permalink":"/blogs/message-expiry","source":"@site/blog/2023-08-27-message-expiry.md","title":"Message expiry","description":"The message expiration is a server-side feature, allowing to automatically delete the old data (expired segment as a whole) depending on the provided configuration. By specifying the custom retention policy, the server can clean up no longer needed messages, which can help with the disk space management.","date":"2023-08-27T00:00:00.000Z","tags":[{"inline":true,"label":"new-features","permalink":"/blogs/tags/new-features"},{"inline":true,"label":"message-expiry","permalink":"/blogs/tags/message-expiry"}],"readingTime":3.76,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Message expiry","slug":"message-expiry","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":["new-features","message-expiry"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Updated schemas","permalink":"/blogs/updated-schemas"},"nextItem":{"title":"Message state","permalink":"/blogs/message-state"}},"content":"The message expiration is a server-side feature, allowing to automatically delete the old data (expired segment as a whole) depending on the provided configuration. By specifying the custom retention policy, the server can clean up no longer needed messages, which can help with the disk space management.\\n\\n\x3c!--truncate--\x3e\\n\\n## Breaking changes\\n\\nNo breaking changes have been introduced with this feature. The only breaking changes are related to the updated `CreateTopic`, `CreateStream` commands and `Stream` + `Topic` responses, which have been introduced in the latest commit [#ea3bf9c](https://github.com/apache/iggy/commit/ea3bf9c16dd5f93e1c80c140e9e1d14cfa70f570). The available [iggy crate](https://crates.io/crates/iggy) supports these changes since version 0.0.50.\\n\\n## Introduction\\n\\nThe expiration policy is defined per topic, and it\'s being set when creating the topic. The value can be either provided via server configuration file, or via `CreateTopic` command. If the expiration is set in the configuration, it will be used as a default value for all the topics, unless when explicitly set when creating the topic via available `message_expiry` field.\\n\\nThe expiration policy is defined as `u32` value, which is the number of seconds after which the message should be marked as expired.\\n\\nFor example, the value of 604800 represents 1 week (60 \\\\* 60 \\\\* 24 \\\\* 7) - after this time, the messages will be marked as expired, and the whole segment will be deleted, as long as all the messages in this segment are expired.\\n\\nIf there are still some messages which are not expired, the segment will be kept on the disk. By default, the new segment is automatically created when the previous one is full - either, it has reached the maximum size (defined by `size_bytes` property) or the currently stored messages have been expired, based on the current server timestamp and configured expiry value.\\n\\nSetting the `message_expiry` to 0 disables the message expiry feature, meaning that the data will be kept forever, unless deleted manually.\\n\\n## Configuration\\n\\n```toml\\n[message_cleaner]\\nenabled = true\\ninterval = 60\\n\\n[system.segment]\\nmessage_expiry = 0\\n```\\n\\n### Message cleaner\\n\\nThe optional component, running in the background, responsible for validating and removing, the expired messages. The whole segment will be removed, when all the messages in this segment are expired. If the `enabled` is set to false, the message cleaner will be disabled, meaning that even if the `message_expiry` is set, the expired messages will not be removed.\\n\\n### Segment\\n\\nThe `message_expiry` option in the `segment` section of the configuration file specifies the number of seconds after which the message will be marked as expired - this is the default value that will be applied to all the topics, unless overridden when creating the topic via API. If the `message_expiry` is set to 0, then the message expiry is disabled.\\n\\n\\n## Commands\\n\\n```rust\\npub struct CreateTopic {\\n    pub stream_id: Identifier,\\n    pub topic_id: u32,\\n    pub partitions_count: u32,\\n    pub message_expiry: Option<u32>,\\n    pub name: String\\n}\\n```\\n\\nThe `message_expiry` field has been added to the `CreateTopic` command, which allows overriding the default value set in the configuration file. If the `message_expiry` is set to None (serialized as 0 value on the binary level), then the message expiry is disabled.\\n\\n```rust\\nfn as_bytes(&self) -> Vec<u8> {\\n    let stream_id_bytes = self.stream_id.as_bytes();\\n    let mut bytes = Vec::with_capacity(13 + stream_id_bytes.len() + self.name.len());\\n    bytes.extend(stream_id_bytes);\\n    bytes.put_u32_le(self.topic_id);\\n    bytes.put_u32_le(self.partitions_count);\\n    match self.message_expiry {\\n        Some(message_expiry) => bytes.put_u32_le(message_expiry),\\n        None => bytes.put_u32_le(0),\\n    }\\n    bytes.put_u8(self.name.len() as u8);\\n    bytes.extend(self.name.as_bytes());\\n    bytes\\n}\\n```\\n\\n\\nAdditionally, the `name_length` field has been added to the binary serialization of the `CreateTopic` and `CreateStream` commands, which allows specifying the length of the name - it should be serialized as `u8` value, before the actual name, with a maximum length of 255 characters.\\n\\nThe following **regex** is used to validate the topic and stream names: `^[\\\\w\\\\.\\\\-\\\\s]+$`. The name will always be lowercased, and the whitespace will be trimmed and then replaced with the `.` character. The following rule applies to all the resources that contain the name field.\\n\\nAlso, the `Topic` and `TopicDetails` returned when fetching the topic(s), have been extended with the `message_expiry` field, and the `name_length` has changed value from `u32 to `u8` (1 byte instead of 4 bytes).\\n\\nSerialization:\\n\\n```\\nTopic ID (4 bytes) + Partitions count (4 bytes) + Message expiry (4 bytes) + Size (8 bytes) + Messages count (8 bytes) + Name length (1 byte) + Name (`Name length` bytes)\\n```\\n\\nThe same changes (`name_length`) apply to the `Stream` and `StreamDetails` structs.\\n\\n```\\nStream ID (4 bytes) + Topics count (4 bytes) + Size (8 bytes) + Messages count (8 bytes) + Name length (1 byte) + Name (`Name length` bytes)\\n```"},{"id":"message-state","metadata":{"permalink":"/blogs/message-state","source":"@site/blog/2023-08-22-message-state.md","title":"Message state","description":"The message state is a simple field which extends the existing message and provides a way to define whether the particular message might be consumed by the client or not, depending on its value. Let\'s briefly discuss the motivation behind this feature, the implementation details and the breaking changes introduced by this release.","date":"2023-08-22T00:00:00.000Z","tags":[{"inline":true,"label":"new-features","permalink":"/blogs/tags/new-features"},{"inline":true,"label":"message-state","permalink":"/blogs/tags/message-state"},{"inline":true,"label":"message-checksum","permalink":"/blogs/tags/message-checksum"}],"readingTime":4.17,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Message state","slug":"message-state","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":["new-features","message-state","message-checksum"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Message expiry","permalink":"/blogs/message-expiry"},"nextItem":{"title":"Message headers","permalink":"/blogs/message-headers"}},"content":"The message state is a simple field which extends the existing `message` and provides a way to define whether the particular message might be consumed by the client or not, depending on its value. Let\'s briefly discuss the motivation behind this feature, the implementation details and the breaking changes introduced by this release.\\n\\n\x3c!--truncate--\x3e\\n\\n## Breaking changes\\n\\nStarting with the commit [#b07f23d](https://github.com/apache/iggy/commit/b07f23db798ffcda7c39650f34547f20137ff725) the breaking changes related to polling the messages by the client have been introduced. The `Message` struct used when invoking `PollMessage` command has been extended with new fields, which results in updated binary schema. The available [iggy crate](https://crates.io/crates/iggy) supports these changes since version 0.0.40.\\n\\n## Introduction\\n\\nThe `state` is an additional field added to the `message` struct stored on the server-side, as well as polled by the client, which describes the current state of the message. By default, the message state is set to `available` (`code = 1``, see the possible codes below), meaning that the message is available to be consumed by the client.\\n\\nHowever, the state might have a different value, for example `marked_for_deletion` (to be used by the upcoming retention policy feature), `poisoned` (to be used by the upcoming dead-letter queue feature) etc. In the future releases, the server will also provide a way to change the state of the message (for example to mark it as `poisoned` by the client), as well as filtering the messages by the state.\\n\\n## Implementation\\n\\n### Structures\\n\\nThe new `MessageState` is an enum which defines the possible states of the message. The `Message` struct has been extended with the `state` field, which is an instance of the `MessageState` enum. The `MessageState` enum also provides a method to convert the enum variant to the `u8` value, which is used by the binary schema.\\n\\nAdditionally, the `Message` struct has been extended with the `checksum` field, which is a `u32` value calculated (**CRC-32** algorithm) from the `payload` field. The checksum is used to verify the integrity of the message, and it\'s calculated on the server-side when the message is being stored. The client can use the checksum to verify the integrity of the message, and if the checksum doesn\'t match, the client can request the message again. This property has been there for a long time now, but it hasn\'t been used by the client so far.\\n\\nThe server and the client now **share the same `Message` struct**, meaning that the same type is being stored on the disk, as well as returned to the client when polling the messages. The motivation behind this approach, was not only to reduce the amount of code, but more importantly to make it easier to implement the extremely important feature being **zero-copy** (copy data from disk directly to the network buffer with bypassing the kernel space). This feature is planned to be implemented in the future.\\n\\n```rust\\npub struct Message {\\n    pub offset: u64,\\n    pub state: MessageState,\\n    pub timestamp: u64,\\n    pub id: u128,\\n    pub checksum: u32,\\n    pub headers: Option<HashMap<HeaderKey, HeaderValue>>,\\n    pub length: u32,\\n    pub payload: Bytes\\n}\\n\\npub enum MessageState {\\n    Available,\\n    Unavailable,\\n    Poisoned,\\n    MarkedForDeletion\\n}\\n\\nimpl MessageState {\\n    pub fn as_code(&self) -> u8 {\\n        match self {\\n            MessageState::Available => 1,\\n            MessageState::Unavailable => 10,\\n            MessageState::Poisoned => 20,\\n            MessageState::MarkedForDeletion => 30\\n        }\\n    }\\n}\\n```\\n\\n### JSON transport\\n\\nAs usual, the JSON transport should be rather easy to implement, as it\'s just a matter of adding the `state` and `checksum` fields to the `message` object being returned when polling the messages. The `state` returns the underscored string value.\\n\\n```json\\n  {\\n    \\"offset\\": 0,\\n    \\"state\\": \\"available\\",\\n    \\"timestamp\\": 1692643862990111,\\n    \\"id\\": 232071677777564499402827199894559175028,\\n    \\"checksum\\": 2144931076,\\n    \\"headers\\": null,\\n    \\"payload\\": \\"b3JkZXJzX2RhdGFfMg==\\"\\n  },\\n  {\\n    \\"offset\\": 1,\\n    \\"state\\": \\"available\\",\\n    \\"timestamp\\": 1692643862990112,\\n    \\"id\\": 44069423551493178892268378627901876657,\\n    \\"checksum\\": 148782482,\\n    \\"headers\\": {\\n      \\"key_3\\": {\\n        \\"kind\\": \\"uint64\\",\\n        \\"value\\": \\"QOIBAAAAAAA=\\"\\n      },\\n      \\"key 1\\": {\\n        \\"kind\\": \\"string\\",\\n        \\"value\\": \\"dmFsdWUx\\"\\n      },\\n      \\"key-2\\": {\\n        \\"kind\\": \\"bool\\",\\n        \\"value\\": \\"AQ==\\"\\n      }\\n    },\\n    \\"payload\\": \\"b3JkZXJzX2RhdGFfMw==\\"\\n  }\\n```\\n\\n### Binary transport\\n\\nNext, let\'s take a look at the binary transport. The `Message` struct has been extended with the `state` and `checksum` fields, which results in the updated binary schema. Previously, the `message` struct was defined as follows:\\n\\n```rust\\npub struct Message {\\n    pub offset: u64,\\n    pub timestamp: u64,\\n    pub id: u128,\\n    pub headers: Option<HashMap<HeaderKey, HeaderValue>>,\\n    pub length: u32,\\n    pub payload: Bytes\\n}\\n```\\n\\nAnd the serialization was:\\n\\n```\\nOffset (8 bytes) + Timestamp (8 bytes) + ID (16 bytes) + Headers (N bytes) + Length (4 bytes) + Payload (`Length` bytes)\\n```\\n\\nNow, the `message` looks like this:\\n\\n```rust\\npub struct Message {\\n    pub offset: u64,\\n    pub state: MessageState,\\n    pub timestamp: u64,\\n    pub id: u128,\\n    pub checksum: u32,\\n    pub headers: Option<HashMap<HeaderKey, HeaderValue>>,\\n    pub length: u32,\\n    pub payload: Bytes\\n}\\n```\\n\\nAnd the serialization has changed to:\\n\\n```\\nOffset (8 bytes) + State (1 byte) + Timestamp (8 bytes) + ID (16 bytes) + Checksum (4 bytes) + Headers (N bytes) + Length (4 bytes) + Payload (`Length` bytes)\\n```\\n\\nAnd that\'s it - simply (de)serialize the `state` field right after the `offset` field, and the `checksum` field right after the `id` field."},{"id":"message-headers","metadata":{"permalink":"/blogs/message-headers","source":"@site/blog/2023-08-19-message-headers.md","title":"Message headers","description":"The optional message headers which you can think of as an additional metadata for your messages have been recently implemented, Let\'s discover what these headers are, how to use them and more importantly, how to implement them in your own transport.","date":"2023-08-19T00:00:00.000Z","tags":[{"inline":true,"label":"new-features","permalink":"/blogs/tags/new-features"},{"inline":true,"label":"message-headers","permalink":"/blogs/tags/message-headers"},{"inline":true,"label":"breaking-changes","permalink":"/blogs/tags/breaking-changes"}],"readingTime":7.36,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Message headers","slug":"message-headers","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":["new-features","message-headers","breaking-changes"],"hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Message state","permalink":"/blogs/message-state"},"nextItem":{"title":"Welcome to the changelog","permalink":"/blogs/welcome-to-the-changelog"}},"content":"The optional message headers which you can think of as an additional metadata for your messages have been recently implemented, Let\'s discover what these headers are, how to use them and more importantly, how to implement them in your own transport.\\n\\n\x3c!--truncate--\x3e\\n\\n## Breaking changes\\n\\nThe first draft of the message headers implementation starts with the initial commit [#bc1d0b3](https://github.com/apache/iggy/commit/bc1d0b369a268783bd329436aba3f6920aae478e). The breaking change to the streaming server has been introduced in the commit [#7c73bd3](https://github.com/apache/iggy/commit/7c73bd3d9d59fb685cbaaca5901dec6cac43ddaf) - up until this point, the existing implementation of the `SendMessages`, `PollMessages` and the underlying file system messages\' storage does work without any breaking changes. The available [iggy crate](https://crates.io/crates/iggy) supports the headers since version 0.0.30.\\n\\n## Introduction\\n\\nMessage headers are an **optional** part of the message which can be used to provide the additional metadata for the message (they are not a part of the message body. Similar to e.g. HTTP headers, you can think of them as a **key-value pairs**, or more precisely the dictionary or map. The headers consist of the key and the value, where the key is a string (**UTF-8 bytes**) and the value is a byte array.\\n\\nSince the headers are optional, you can send a message without any headers at all. There\'s no limit on the number of headers you can send, but the total size of the headers is currently limited to **100 KB**. The header key is **case-sensitive**, so it\'s best to send the **lowercase** keys. The maximum length of the key and the value is **255 bytes** (the maximum value of the `u8` type).\\n\\nFor now, there are no reserved headers, so you can use any key you want. However, in the future, we might introduce some reserved headers used by the streaming server for the specific purposes such as the message compression, distributed tracing and so no.\\n\\nThe **sample applications** using the message headers can be found [here](https://github.com/apache/iggy/tree/master/core/examples/src/message-headers).\\n\\n## Implementation\\n\\n### Structures\\n\\nWe will start with the sample Rust implementation of the headers. The headers are defined as a `HashMap` where the key is a `string` (wrapped with a custom `HeaderKey` struct to enforce the validation) and the value is a `HeaderValue` struct. The `HeaderValue` struct consists of the `HeaderKind` enum and the `Vec<u8>` value. The `HeaderKind` enum defines the type of the value, which can be either a raw byte array or one of the primitive types.\\n\\nThe primitive types are defined as the enum variants, so you can easily match on them. The `HeaderKind` enum is used to validate the value and to serialize/deserialize it. The idea is that the value will always remain a byte array, but you can use the `HeaderKind` to convert it to the desired type.\\n\\n```rust\\npub struct HeaderKey(String);\\n\\npub struct HeaderValue {\\n    pub kind: HeaderKind,\\n    pub value: Vec<u8>\\n}\\n\\npub enum HeaderKind {\\n    Raw,\\n    String,\\n    Bool,\\n    Int8,\\n    Int16,\\n    Int32,\\n    Int64,\\n    Int128,\\n    Uint8,\\n    Uint16,\\n    Uint32,\\n    Uint64,\\n    Uint128,\\n    Float32,\\n    Float64\\n}\\n```\\n\\nNext, let\'s take a look at the `SendMessages` command, and in particular we will see what has changed in the underlying `Message` struct (keep in mind that headers are unique per message, so they are not a part of the `SendMessages` command). The `Message` struct now has an additional `headers` field, which is a `HashMap` of `HeaderKey` and `HeaderValue` structs:\\n\\n```rust\\npub struct SendMessages {\\n    pub stream_id: Identifier,\\n    pub topic_id: Identifier,\\n    pub partitioning: Partitioning,\\n    pub messages: Vec<Message>\\n}\\n\\npub struct Message {\\n    pub id: u128,\\n    pub length: u32,\\n    pub payload: Bytes,\\n    pub headers: Option<HashMap<HeaderKey, HeaderValue>>\\n}\\n```\\n\\n### JSON transport\\n\\nHeaders are optional, so depending on the programming language, you can either use the `option` type, or the `null` value, or even instantiate an `empty` map - it\'s up to you. The important part is the serialization. For the JSON transport, it\'s very easy, as you can see in the following example:\\n\\n```json\\n{\\n  \\"partitioning\\": {\\n    \\"kind\\": \\"partition_id\\",\\n    \\"value\\": \\"{{partition_id_payload_base64}}\\"\\n  },\\n  \\"messages\\": [{\\n    \\"id\\": 0,\\n    \\"payload\\": \\"{{message_1_payload_base64}}\\"\\n  }, {\\n    \\"id\\": 0,\\n    \\"payload\\": \\"{{message_2_payload_base64}}\\",\\n    \\"headers\\": {\\n      \\"key_1\\": {\\n        \\"kind\\": \\"string\\",\\n        \\"value\\": \\"{{header_1_payload_base_64}}\\"\\n      }\\n    }\\n  }]\\n}\\n```\\n\\nSimply put, in order to include the headers in the message, you need to add the `headers` field to the message struct. The `headers` field is a map of the `String` keys and the `HeaderValue` values. The `HeaderValue` value is a JSON object which consists of the `kind` and the `value` fields. The `kind` field is a string which defines the type of the value, and the `value` field is a base64-encoded byte array.\\n\\n### Binary transport\\n\\nNow, we can move on to the more complex part, being the binary transport (as always, it\'s the same schema for both, TCP and QUIC protocol).\\n\\nWe will begin with the headers serialization for the whole `HashMap`:\\n\\n```rust\\nimpl BytesSerializable for HashMap<HeaderKey, HeaderValue> {\\n    fn as_bytes(&self) -> Vec<u8> {\\n        if self.is_empty() {\\n            return EMPTY_BYTES;\\n        }\\n\\n        let mut bytes = vec![];\\n        for (key, value) in self {\\n            bytes.put_u32_le(key.0.len() as u32);\\n            bytes.extend(key.0.as_bytes());\\n            bytes.put_u8(value.kind.as_code());\\n            bytes.put_u32_le(value.value.len() as u32);\\n            bytes.extend(&value.value);\\n        }\\n\\n        bytes\\n    }\\n}\\n```\\n\\nIf for some reason, there are no headers, we return an empty byte array. Otherwise, we iterate over the headers and serialize them one by one. The serialization shouldn\'t be too difficult as we start with the key length, then we add the key itself (**UTF-8 bytes**), then we add the value type (the `HeaderKind` enum variant), then we add the value length, and finally we add the value itself (**UTF-8 bytes**).\\n\\nThe length of the single header is the following: `4 bytes` for the key length, `n bytes` (**1-255**) for the key, `1 byte` for the value kind, `4 bytes` for the value length and `n bytes` (**1-255**) for the value itself.\\n\\nThe `HeaderKind` uses the following codes:\\n\\n```rust\\nimpl HeaderKind {\\n    pub fn as_code(&self) -> u8 {\\n        match self {\\n            HeaderKind::Raw => 1,\\n            HeaderKind::String => 2,\\n            HeaderKind::Bool => 3,\\n            HeaderKind::Int8 => 4,\\n            HeaderKind::Int16 => 5,\\n            HeaderKind::Int32 => 6,\\n            HeaderKind::Int64 => 7,\\n            HeaderKind::Int128 => 8,\\n            HeaderKind::Uint8 => 9,\\n            HeaderKind::Uint16 => 10,\\n            HeaderKind::Uint32 => 11,\\n            HeaderKind::Uint64 => 12,\\n            HeaderKind::Uint128 => 13,\\n            HeaderKind::Float32 => 14,\\n            HeaderKind::Float64 => 15,\\n        }\\n    }\\n}\\n```\\n\\n### Sending the messages with headers\\n\\nNow, let\'s see how we can include the headers in a message serialization:\\n\\n```rust\\nfn as_bytes(&self) -> Vec<u8> {\\n    let mut bytes = Vec::with_capacity(self.get_size_bytes() as usize);\\n    bytes.put_u128_le(self.id);\\n    if let Some(headers) = &self.headers {\\n        let headers_bytes = headers.as_bytes();\\n        bytes.put_u32_le(headers_bytes.len() as u32);\\n        bytes.extend(&headers_bytes);\\n    } else {\\n        bytes.put_u32_le(0);\\n    }\\n    bytes.put_u32_le(self.length);\\n    bytes.extend(&self.payload);\\n    bytes\\n}\\n```\\n\\nThe important thing to notice, is that the **headers are being serialized right after the `ID` and before the actual message payload**. At first, we add the total length of the headers\' payload, then we add the headers themselves, and finally we include the message payload (length + value) as before. If the headers are not provided at all (e.g. `none` or `null` or an `empty` array), we add the `0` value as the headers\' length, and we skip the headers\' serialization, since there are no headers (an empty byte array).\\n\\n\\n### Polling the messages with headers\\n\\nEventually, let\'s discuss the `PollMessages` command. Nothing has changed here, except of the extended `Message` struct, which now contains the optional headers. For the JSON transport it\'s the same as when sending the messages, and for the binary transport, we need to deserialize the headers in the same way as we did when sending the messages. Let\'s find out how it works.\\n\\n\\n```rust\\npub fn map_message(&self, bytes: &mut Vec<u8>) {\\n    bytes.put_u64_le(self.offset);\\n    bytes.put_u64_le(self.timestamp);\\n    bytes.put_u128_le(self.id);\\n    if let Some(headers) = &self.headers {\\n        let headers_bytes = headers.as_bytes();\\n        bytes.put_u32_le(headers_bytes.len() as u32);\\n        bytes.extend(&headers_bytes);\\n    } else {\\n        bytes.put_u32_le(0u32);\\n    }\\n    bytes.put_u32_le(self.length);\\n    bytes.extend(&self.payload);\\n}\\n```\\n\\nAs you can see, the headers are being serialized in the same way as when sending the messages. The only difference is that the `Message` struct also contains the `offset` and the `timestamp` fields, which are being serialized before the `ID` field. Besides that, nothing has changed, as we simply include the headers right after the `ID` field and before the actual message payload.\\n\\n### Sample usage\\n\\nThe headers usage is pretty straightforward, as we simply need to create the new `HashMap`:\\n\\n```rust\\nlet mut headers = HashMap::new();\\nheaders.insert(HeaderKey::new(\\"key 1\\")?, HeaderValue::from_str(\\"value1\\")?);\\nheaders.insert(HeaderKey::new(\\"key-2\\")?, HeaderValue::from_bool(true)?);\\nheaders.insert(HeaderKey::new(\\"key_3\\")?, HeaderValue::from_uint64(123456)?);\\n```\\n\\nYou might also use the conversion based on the implemented traits:\\n\\n```rust\\nheaders.insert(\\"key\\".try_into()?, HeaderValue::from_float64(123.45)?);\\nlet value = headers.get(&\\"key\\".try_into()?)?.as_float64()?;\\n```\\n\\nKeep in mind, that the `HeaderKey` and the `HeaderValue` structs are being validated whenever you use one of the available methods to create them.\\n\\nAnd then, we can send the messages with the headers (first `none` argument is for the optional ID):\\n\\n```rust\\nlet send_messages = SendMessages {\\n    stream_id: Identifier::numeric(1)?,\\n    topic_id: Identifier::named(\\"sample\\")?,\\n    partitioning: Partitioning::balanced(),\\n    messages: vec![Message::new(None, Bytes::from(\\"hello-world\\") , Some(headers))]\\n};\\n```\\n\\nWhen you poll the messages and want to read the header value based on its kind, you can also use the existing helper methods, for example:\\n\\n```rust\\nlet value1 = headers.get(&HeaderKey::new(\\"key 1\\")?)?.as_str()?;\\nlet value2 = headers.get(&HeaderKey::new(\\"key-2\\")?)?.as_bool()?;\\nlet value3 = headers.get(&HeaderKey::new(\\"key_3\\")?)?.as_uint64()?;\\n```"},{"id":"welcome-to-the-changelog","metadata":{"permalink":"/blogs/welcome-to-the-changelog","source":"@site/blog/2023-08-16-welcome.md","title":"Welcome to the changelog","description":"Welcome to the changelog, in which you will read about all the changes that we make to the project.","date":"2023-08-16T00:00:00.000Z","tags":[{"inline":true,"label":"hello","permalink":"/blogs/tags/hello"}],"readingTime":0.515,"hasTruncateMarker":true,"authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz","socials":{},"key":null,"page":null}],"frontMatter":{"title":"Welcome to the changelog","slug":"welcome-to-the-changelog","authors":[{"name":"Piotr Gankiewicz","title":"Apache Iggy founder","url":"https://github.com/spetz"}],"tags":["hello"],"hide_table_of_contents":false,"date":"2023-08-16T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Message headers","permalink":"/blogs/message-headers"}},"content":"Welcome to the changelog, in which you will read about all the changes that we make to the project.\\n\\n\x3c!--truncate--\x3e\\n\\nFor the time being, I haven\'t documented all the changes that I\'ve made to the project, but since there\'s already a few of us working on the different parts of the project, I think it\'s a good idea to start documenting them, especially when there are some breaking changes or the new features being added to the server, that should be implemented in the client SDK as well :)\\n\\nStay tuned for the updates and the upcoming features, such as optional message headers!"}]}}')}}]);